{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-03-07T14:34:34.653535Z","iopub.status.busy":"2024-03-07T14:34:34.652850Z","iopub.status.idle":"2024-03-07T14:34:40.781469Z","shell.execute_reply":"2024-03-07T14:34:40.780703Z","shell.execute_reply.started":"2024-03-07T14:34:34.653491Z"},"trusted":true},"outputs":[],"source":["from accelerate.utils import BnbQuantizationConfig\n","from accelerate import Accelerator, notebook_launcher\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, \\\n","                        get_cosine_schedule_with_warmup, set_seed\n","import transformers\n","import optimum\n","\n","from datasets import load_dataset,Dataset\n","import pandas as pd\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, \\\n","                        get_cosine_schedule_with_warmup, set_seed\n","from peft import LoraConfig, TaskType, get_peft_model\n","from accelerate import Accelerator, notebook_launcher\n","from accelerate.utils import set_seed\n","import logging\n","from torch.utils.data import DataLoader\n","from torch.optim import AdamW, SGD\n","from tqdm.notebook import tqdm\n","import torch\n","from torch.nn.utils.rnn import pad_sequence\n","import glob\n","from collections import OrderedDict\n","import re\n","\n","import os"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:34:40.783110Z","iopub.status.busy":"2024-03-07T14:34:40.782634Z","iopub.status.idle":"2024-03-07T14:34:40.787187Z","shell.execute_reply":"2024-03-07T14:34:40.786265Z","shell.execute_reply.started":"2024-03-07T14:34:40.783080Z"},"trusted":true},"outputs":[],"source":["import datetime\n","start_time = datetime.datetime.now()"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:34:40.790460Z","iopub.status.busy":"2024-03-07T14:34:40.790109Z","iopub.status.idle":"2024-03-07T14:34:40.800232Z","shell.execute_reply":"2024-03-07T14:34:40.799424Z","shell.execute_reply.started":"2024-03-07T14:34:40.790431Z"},"trusted":true},"outputs":[],"source":["def truncate_txt(text, length):\n","    text_list = text.split()\n","    \n","    if len(text_list) <= length:\n","        return text\n","    \n","    return \" \".join(text_list[:length])\n","\n","\n","def gen_prompt(og_text, rewritten_text):\n","    \n","    # Truncate the texts to first 200 words for now\n","    # As we are having memory issues on Mixtral8x7b\n","    og_text = truncate_txt(og_text, 170)\n","    rewritten_text = truncate_txt(rewritten_text, 170)\n","    \n","    return f\"\"\"    \n","    \n","    You are given 2 essays, the Rewritten text was created from the Original text using the google Gemma model.Analyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten text. Start directly with the prompt.\n","    \n","    Original Text:\n","    \\\"\"\"{og_text}\\\"\"\"\n","    \n","    Rewritten Text:\n","    \\\"\"\"{rewritten_text}\\\"\"\"\n","    \n","    Prompt:\n","    \"\"\".strip()"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:34:40.803388Z","iopub.status.busy":"2024-03-07T14:34:40.803090Z","iopub.status.idle":"2024-03-07T14:34:42.359073Z","shell.execute_reply":"2024-03-07T14:34:42.358255Z","shell.execute_reply.started":"2024-03-07T14:34:40.803366Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","filename1 = \"./input/gemma-rewrite-nbroad/nbroad-v1.csv\"\n","filename2 = \"./input/gemma-rewrite-nbroad/nbroad-v2.csv\"\n","df = pd.concat([pd.read_csv(filename1), pd.read_csv(filename2)]).reset_index(drop=True)\n","\n","df['reverse_prompt'] = df.apply(lambda x: gen_prompt(x.original_text, x.rewritten_text), axis=1)\n","df = df.iloc[:6]\n","\n","data = Dataset.from_pandas(df[['reverse_prompt', 'rewrite_prompt']],split='train')"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:34:42.360542Z","iopub.status.busy":"2024-03-07T14:34:42.360264Z","iopub.status.idle":"2024-03-07T14:34:46.357229Z","shell.execute_reply":"2024-03-07T14:34:46.356439Z","shell.execute_reply.started":"2024-03-07T14:34:42.360518Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e8ce9ebfab844c83bf4a123a65852ffb","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/6 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# MODEL_PATH = \"/kaggle/input/gemma/transformers/7b-it/2\"\n","# MODEL_PATH = \"distilbert/distilroberta-base\"\n","MODEL_PATH = \"distilbert/distilgpt2\"\n","# MODEL_PATH = \"gpt2\"\n","# MODEL_PATH = 'mistralai/Mistral-7B-Instruct-v0.2'\n","# MODEL_PATH = \"google/gemma-2b-it\"\n","# MODEL_PATH = \"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\"\n","# MODEL_PATH = \"/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1\"\n","# MODEL_PATH = \"/kaggle/input/llama-2/pytorch/7b-chat-hf/1\"\n","# MODEL_PATH = \"/kaggle/input/llama-2/pytorch/13b-chat-hf/1\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n","def tokenize_samples(samples):\n","    inputs = tokenizer(samples[\"reverse_prompt\"], max_length=512, truncation=True)\n","    targets = tokenizer(samples[\"rewrite_prompt\"], max_length=512, truncation=True)\n","    return {'input_ids': inputs['input_ids'], 'attention_mask': inputs['attention_mask'], 'labels': targets['input_ids']}\n","    \n","data = data.map(tokenize_samples, batched=True)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# tokenizer.model_max_length = 512"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:34:46.358636Z","iopub.status.busy":"2024-03-07T14:34:46.358351Z","iopub.status.idle":"2024-03-07T14:34:46.367556Z","shell.execute_reply":"2024-03-07T14:34:46.366644Z","shell.execute_reply.started":"2024-03-07T14:34:46.358612Z"},"trusted":true},"outputs":[],"source":["def collate_fn(batch):\n","    inputs = [torch.tensor(b['input_ids']) for b in batch]\n","    labels = [torch.tensor(b['labels']) for b in batch]\n","\n","    max_length = max(len(input_) for input_ in inputs)\n","\n","    input_ids = pad_sequence(inputs, batch_first=True, padding_value=tokenizer.pad_token_id)\n","\n","    padded_labels = []\n","    for label in labels:\n","        padded_label = torch.full((max_length,), fill_value=-100, dtype=torch.long)\n","        padded_label[:len(label)] = label\n","        padded_labels.append(padded_label)\n","    padded_labels = torch.stack(padded_labels)\n","\n","    attention_mask = torch.ones(input_ids.shape, dtype=torch.long)\n","    attention_mask[input_ids == tokenizer.pad_token_id] = 0\n","\n","    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': padded_labels}\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:34:46.368907Z","iopub.status.busy":"2024-03-07T14:34:46.368641Z","iopub.status.idle":"2024-03-07T14:34:46.378598Z","shell.execute_reply":"2024-03-07T14:34:46.377738Z","shell.execute_reply.started":"2024-03-07T14:34:46.368881Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<function collate_fn at 0x733b20b4bf40>\n"]}],"source":["print(collate_fn)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:34:46.379858Z","iopub.status.busy":"2024-03-07T14:34:46.379580Z","iopub.status.idle":"2024-03-07T14:34:46.388744Z","shell.execute_reply":"2024-03-07T14:34:46.387945Z","shell.execute_reply.started":"2024-03-07T14:34:46.379832Z"},"trusted":true},"outputs":[],"source":["# import torch\n","# from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig\n","\n","\n","# # Found a good blog to catch me up fast!\n","# # https://huggingface.co/blog/4bit-transformers-bitsandbytes\n","# # https://huggingface.co/docs/transformers/v4.38.1/en/quantization#compute-data-type\n","# quantization_config = BitsAndBytesConfig(\n","#     load_in_4bit = True,\n","#     bnb_4bit_quant_type=\"nf4\",\n","#     bnb_4bit_compute_dtype=torch.bfloat16,\n","#     bnb_4bit_use_double_quant=True,\n","# )\n","\n","\n","\n","# model = AutoModelForCausalLM.from_pretrained(\n","#     MODEL_PATH,\n","#     device_map = \"auto\",\n","#     trust_remote_code = True,\n","#     quantization_config=quantization_config,\n","# )\n","\n","# # model = model.to_bettertransformer()\n","# model = accelerator.prepare(model)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:34:46.403559Z","iopub.status.busy":"2024-03-07T14:34:46.403334Z","iopub.status.idle":"2024-03-07T14:34:46.415815Z","shell.execute_reply":"2024-03-07T14:34:46.414865Z","shell.execute_reply.started":"2024-03-07T14:34:46.403539Z"},"trusted":true},"outputs":[],"source":["def del_past_models(save_path, file_exten='pth'):\n","    \"\"\"\n","    Remove all of the past models\n","    \n","    You can change the file_extension if you save the models with other file_extension\n","    \"\"\"\n","    past_models = glob.glob(os.path.join(save_path, '*.' + file_exten))\n","    for past_model in past_models:\n","        os.remove(past_model)\n","        logger.info(f'Remove model {past_model}!')\n","        \n","def save_checkpoint(path, model, optim, sched, epoch, iters):\n","    lr = optim.param_groups[0]['lr']\n","    # model_state = model.state_dict()\n","    model_state = OrderedDict((name, param) for name, param in model.named_parameters() \\\n","                    if param.requires_grad)\n","    \n","    checkpoint_path = os.path.join(path, f'checkpoint_{iters + 1}.pth')\n","    new_checkpoint_path = os.path.join(path, f'checkpoint.pth')\n","    logger.info(f\"Model of epoch {epoch} saved at checkpoint_{iters + 1}.pth, lr={lr:.3e}\")\n","    \n","    torch.save({\n","        'model': model_state,\n","        'optimizer': optim.state_dict(),\n","        'scheduler': sched.state_dict(), \n","        'epoch': epoch\n","    }, checkpoint_path)\n","    \n","    torch.save({\n","        'model': model_state,\n","        'optimizer': optim.state_dict(),\n","        'scheduler': sched.state_dict(), \n","        'epoch': epoch\n","    }, new_checkpoint_path)\n","\n","    \n","def load_checkpoint(checkpoint_path, model, optim=None, sched=None):\n","    checkpoint = torch.load(checkpoint_path)\n","    logger.info(f\"Model of epoch {checkpoint['epoch']} is loaded\")\n","    \n","    model.load_state_dict(checkpoint['model'], strict=False)\n","    if optim is not None and sched is not None:\n","        optim.load_state_dict(checkpoint['optimizer'])\n","        sched.load_state_dict(checkpoint['scheduler'])\n","        return model, optim, sched, checkpoint['epoch']\n","    else:\n","        return model, checkpoint['epoch']"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:41:09.200753Z","iopub.status.busy":"2024-03-07T14:41:09.200368Z","iopub.status.idle":"2024-03-07T14:41:09.223448Z","shell.execute_reply":"2024-03-07T14:41:09.222511Z","shell.execute_reply.started":"2024-03-07T14:41:09.200725Z"},"trusted":true},"outputs":[],"source":["def train_epoch(epoch, model, accelerator, \n","                train_dataloader, checkpointing_steps, \n","                optimizer, lr_scheduler, save_path):\n","    global overall_step\n","    model.train()\n","    epoch_loss = []\n","    pbar = tqdm(train_dataloader)\n","    grad = torch.tensor(0.0)\n","    \n","    output_dir = f\"step_{overall_step}.pth\"\n","    unwrapped_model = accelerator.unwrap_model(model)\n","    save_checkpoint(save_path, unwrapped_model, optimizer, lr_scheduler, \n","                   epoch, overall_step)\n","    \n","    for step, batch in enumerate(pbar):\n","        if (datetime.datetime.now() - start_time) > datetime.timedelta(hours=6):\n","            break\n","        with accelerator.accumulate(model):\n","            # Gradient accumulation\n","            # with accelerator.autocast():\n","            outputs = model(**batch)\n","            loss = outputs.loss\n","            accelerator.backward(loss)\n","            if accelerator.sync_gradients:\n","                grad = accelerator.clip_grad_norm_(parameters=model.parameters(), \n","                                                   max_norm=2.0)\n","            optimizer.step()\n","            lr_scheduler.step()\n","            optimizer.zero_grad()\n","\n","        overall_step += 1\n","        lr = optimizer.param_groups[0]['lr']\n","        \n","        pbar.set_description(\n","            f'Epoch {epoch}: loss = {loss.item(): .3f}, grad = {grad.item(): .3f}, lr = {lr: .3e}')\n","        \n","        with torch.no_grad():\n","            avg_loss = accelerator.gather(loss.repeat(len(batch))).mean()\n","        epoch_loss.append(avg_loss.item() / accelerator.gradient_accumulation_steps)\n","        \n","        accelerator.wait_for_everyone()\n","        if overall_step % checkpointing_steps == 0:\n","            logger.info(\n","                f'Epoch {epoch}: loss = {loss.item(): .3f}, grad = {grad.item(): .3f}, lr = {lr: .3e}')\n","            if accelerator.is_local_main_process:\n","                # Clear all of the current models\n","                del_past_models(save_path)\n","\n","                output_dir = f\"step_{overall_step}.pth\"\n","                unwrapped_model = accelerator.unwrap_model(model)\n","                save_checkpoint(save_path, unwrapped_model, optimizer, lr_scheduler, \n","                               epoch, overall_step)\n","    \n","    # Just log the loss of the main process\n","    if len(epoch_loss) > 0:\n","        logger.info(f'Epoch {epoch}: loss = {sum(epoch_loss) / (len(epoch_loss)+1e-10): .3f}, lr = {lr: .3e}')\n","    \n","            \n","def main(batch_size: int, num_epochs: int, lr: float, grad_accumulation_steps: int, \n","         checkpointing_steps: int, save_path: str, ckpt_path: str,\n","         num_warmup_steps: int=0, r: int=4, lora_alpha: int=32, lora_dropout: float=0.1):\n","    set_seed(1234)\n","    \n","    accelerator = Accelerator(gradient_accumulation_steps=grad_accumulation_steps)\n","    quantization_config = BitsAndBytesConfig(\n","        load_in_4bit=True, \n","        bnb_4bit_quant_type=\"nf4\",\n","        bnb_4bit_compute_dtype=torch.bfloat16,\n","        bnb_4bit_use_double_quant=True,\n","    )\n","    \n","    model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, \n","                                                 quantization_config=quantization_config, \n","                                                 torch_dtype=torch.float16)\n","    model.save_pretrained(save_path)\n","    tokenizer.save_pretrained(save_path)\n","    \n","    \n","    accelerator.print(model)\n","    \n","    # Instantiate dataloaders. (We do not split the test data)\n","    train_dataloader = DataLoader(\n","        data, shuffle=True, collate_fn=collate_fn, batch_size=batch_size\n","    )\n","    \n","    peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, \n","                             inference_mode=False, r=r, \n","                             lora_alpha=lora_alpha, lora_dropout=lora_dropout,\n","                             target_modules=\n","                            #  [\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n","                            ['c_attn', 'c_proj', 'c_fc'],\n","                            )\n","    model = get_peft_model(model, peft_config)\n","    \n","    if accelerator.is_local_main_process:\n","        model.print_trainable_parameters()\n","    \n","    lr = lr * accelerator.num_processes * accelerator.gradient_accumulation_steps\n","    \n","    optimizer = AdamW(params=model.parameters(), lr=lr)\n","    \n","    total_steps = len(train_dataloader) * num_epochs * \\\n","            accelerator.num_processes * accelerator.gradient_accumulation_steps\n","    \n","    # Instantiate scheduler\n","    lr_scheduler = get_cosine_schedule_with_warmup(\n","        optimizer=optimizer,\n","        num_warmup_steps=num_warmup_steps,\n","        # to ensure the lr will not become zero at the end of training\n","        # (You can adjust this param)\n","        num_training_steps=total_steps * 1.1,\n","    )\n","    \n","    current_epochs = 0\n","    global overall_step\n","    overall_step = 0\n","    \n","    # Load checkpoint\n","    print(ckpt_path)\n","    if os.path.isfile(ckpt_path):\n","        print('loading')\n","        model, optimizer, lr_scheduler, current_epochs = \\\n","                    load_checkpoint(ckpt_path, model, optimizer, lr_scheduler)\n","        overall_step = int(re.search('(\\d)+', ckpt_path).group())\n","        logger.info(\n","            f'Checkpoint {ckpt_path} loaded at epoch {current_epochs}, the training will resume from epoch {current_epochs + 1}!')\n","        current_epochs += 1\n","    \n","    \n","    if current_epochs >= num_epochs:\n","        raise ValueError('The num_epochs should be larger than the saved epochs!!')\n","    \n","    # Prepare everything\n","    # There is no specific order to remember, \n","    # we just need to unpack the objects in the same order we gave them to the prepare method.\n","    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n","        model, optimizer, train_dataloader, lr_scheduler\n","    )\n","    \n","    \n","    logger.info('*********************** Start training! **************************')\n","    \n","    for epoch in range(current_epochs, num_epochs):\n","        train_epoch(epoch, model, accelerator, train_dataloader, \n","                    checkpointing_steps, optimizer, lr_scheduler, save_path)\n","        \n","    # Save the final model\n","    accelerator.wait_for_everyone()\n","    if accelerator.is_local_main_process:\n","        output_dir = f\"step_{overall_step}.pth\"\n","        unwrapped_model = accelerator.unwrap_model(model)\n","        save_checkpoint(save_path, unwrapped_model, optimizer, lr_scheduler, \n","                       epoch, overall_step)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:34:46.442375Z","iopub.status.busy":"2024-03-07T14:34:46.442144Z","iopub.status.idle":"2024-03-07T14:34:46.455275Z","shell.execute_reply":"2024-03-07T14:34:46.454417Z","shell.execute_reply.started":"2024-03-07T14:34:46.442355Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["50256\n"]}],"source":["if tokenizer.pad_token_id is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","print(tokenizer.pad_token_id)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["train_dataloader = DataLoader(\n","    data, shuffle=True, collate_fn=collate_fn, batch_size=2\n",")"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["ckpt_path = './working/trained_models/%s/checkpoint.pth' % MODEL_PATH"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["lora_target_modules_dict = {\n","  'gpt2': ['c_attn'],\n","  'distilbert/distilgpt2': ['c_attn'],\n","  'distilbert/distilroberta-base': ['query', 'key', 'value'],\n","}\n","import json\n","os.makedirs('./settings', exist_ok=True)\n","json.dump(lora_target_modules_dict, open('./settings/lora_target_modules.json', 'w'))"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"]}],"source":["quantization_config = BitsAndBytesConfig(\n","  # load_in_4bit=True, \n","  # bnb_4bit_quant_type=\"nf4\",\n","  # bnb_4bit_compute_dtype=torch.bfloat16,\n","  # bnb_4bit_use_double_quant=True,\n",")\n","\n","model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, \n","                                            quantization_config=quantization_config, \n","                                            torch_dtype=torch.float16)\n","\n","peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, \n","                          inference_mode=False, r=64, \n","                          lora_alpha=64, lora_dropout=0.1,\n","                          target_modules=\n","                         lora_target_modules_dict.get(MODEL_PATH, [\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]),\n","                        )\n","model = get_peft_model(model, peft_config)\n","\n","logger=logging.getLogger(__name__)\n","\n","# model, _ = load_checkpoint(ckpt_path, model)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/media/minhduck/SSD1/Documents/projects/pyenvs/rlhf/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:226: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n","  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.')\n"]}],"source":["batch = list(iter(train_dataloader))[1]\n","\n","model.eval()\n","model.to('cuda')\n","with torch.no_grad():\n","  output = model(**batch)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(11.8203, dtype=torch.float16)"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["output.loss"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["You are given 2 essays, the Rewritten text was created from the Original text using the google Gemma model.Analyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten text. Start directly with the prompt.\n","    \n","    Original Text:\n","    \"\"\"I can hardly read the letter, because the hand holding it is shaking so bad. My hand. There's nothing wrong with me, I'm just nervous. I'm actually a pretty normal guy, painfully normal. But that's all about to change. *Dear Mr. Thompson, we are quite happy to inform you that we have successfully received and processed your requests, as well as your official test results. It is with great pleasure that I can relay to you, on behalf of The United States Department Of Supernatural Distribution, that you are officially licensed to generation and application of banishment, disappearance, and-* The sturdy stationery and pompous print fade from view. Everything is temporarily blurry, and I feel my nose flare and quiver for a moment, before the rest of my body joins in. I'm not much of a crier, and make a pathetic scene for a moment as I let the emotion wash over me. Finally. Finally. Fucking finally. I blink the tears out of my eyes, and hear the soft thud\"\"\"\n","    \n","    Rewritten Text:\n","    \"\"\"My hand quivered as I clutched the letter, the parchment trembling beneath my grip. It was an official letter, bearing the seal of an agency I had never heard of – The United States Department Of Supernatural Distribution. The words danced across the page, promising me the power to dispel, vanish, and transport. My heart raced with anticipation, a cocktail of fear and exhilaration. The letter detailed my official licensing to generate and apply \"banishment, disappearance, and transmigration\" – powers hitherto unimaginable. The words \"banishment, disappearance, transmigration\" echoed in my mind like a siren's song, promising me the ability to manipulate reality itself. The paper burned my eyes, and my nose twitched in anticipation of the intoxicating scent of power that I was about to wield. I read the code, my voice cracking with emotion, and the air crackled with anticipation. As I waved my hand and focused on the document, it vanished. It was as if I had conj\n"]}],"source":["prompt = tokenizer.decode(batch['input_ids'][0])\n","print(prompt)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["\n","No chat template is defined for this tokenizer - using the default template for the GPT2TokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n","\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"]},{"data":{"text/plain":["Conversation id: c28e9313-3e3c-4235-819d-a84cf4c66104\n","user: You are given 2 essays, the Rewritten text was created from the Original text using the google Gemma model.Analyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten text. Start directly with the prompt.\n","    \n","    Original Text:\n","    \"\"\"I can hardly read the letter, because the hand holding it is shaking so bad. My hand. There's nothing wrong with me, I'm just nervous. I'm actually a pretty normal guy, painfully normal. But that's all about to change. *Dear Mr. Thompson, we are quite happy to inform you that we have successfully received and processed your requests, as well as your official test results. It is with great pleasure that I can relay to you, on behalf of The United States Department Of Supernatural Distribution, that you are officially licensed to generation and application of banishment, disappearance, and-* The sturdy stationery and pompous print fade from view. Everything is temporarily blurry, and I feel my nose flare and quiver for a moment, before the rest of my body joins in. I'm not much of a crier, and make a pathetic scene for a moment as I let the emotion wash over me. Finally. Finally. Fucking finally. I blink the tears out of my eyes, and hear the soft thud\"\"\"\n","    \n","    Rewritten Text:\n","    \"\"\"My hand quivered as I clutched the letter, the parchment trembling beneath my grip. It was an official letter, bearing the seal of an agency I had never heard of – The United States Department Of Supernatural Distribution. The words danced across the page, promising me the power to dispel, vanish, and transport. My heart raced with anticipation, a cocktail of fear and exhilaration. The letter detailed my official licensing to generate and apply \"banishment, disappearance, and transmigration\" – powers hitherto unimaginable. The words \"banishment, disappearance, transmigration\" echoed in my mind like a siren's song, promising me the ability to manipulate reality itself. The paper burned my eyes, and my nose twitched in anticipation of the intoxicating scent of power that I was about to wield. I read the code, my voice cracking with emotion, and the air crackled with anticipation. As I waved my hand and focused on the document, it vanished. It was as if I had conj\n","assistant: The first time I've been in the world of the internet, I've been using the internet for a while now. I've been using the internet for a while now. I've been using the internet for a while now. I've been using the internet for a while now. I've been using the internet for a while now. I've been using the internet for a while now. I've been using the internet for a while now. I've been using the internet for a while now. I've been using the internet for a while now. I've been using the internet for a while now. I've been using the internet for a while now. I've been using the internet for a while now. I've been using the internet for a while now. I've been using the internet for a while now. I've been using the internet for a while now. I've been using the internet for a while now. I've been using the internet for a while now. I've been using the internet for a while now. I've been using the internet for a while now. I've been using the internet for a while now. I've been using the internet for a while now. I've been using the internet for a while now. I"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import pipeline, Conversation\n","prompt = tokenizer.decode(batch['input_ids'][0])\n","\n","pp = pipeline(task='conversational', model=model, tokenizer=tokenizer)\n","\n","pp(Conversation(prompt))"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"]}],"source":["prompt = tokenizer.decode(batch['input_ids'][0])\n","messages = [\n","    {\n","        \"role\": \"user\",\n","        \"content\": prompt\n","    }\n","]\n","encoded_input = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to('cuda:0')\n","with torch.no_grad():\n","    encoded_output = model.generate(encoded_input, max_new_tokens=128, do_sample=True, pad_token_id=tokenizer.eos_token_id, num_beams=2, top_k=20)\n","    decoded_output = tokenizer.decode(encoded_output[0], skip_special_tokens=True).replace(prompt, '').replace(\"[INST]\", \"\").replace(\"[/INST]\", \"\").strip()"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"data":{"text/plain":["'You are given 2 essays, the Rewritten text was created from the Original text using the google Gemma model.Analyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten text. Start directly with the prompt.\\n    \\n    Original Text:\\n    \"\"\"I can hardly read the letter, because the hand holding it is shaking so bad. My hand. There\\'s nothing wrong with me, I\\'m just nervous. I\\'m actually a pretty normal guy, painfully normal. But that\\'s all about to change. *Dear Mr. Thompson, we are quite happy to inform you that we have successfully received and processed your requests, as well as your official test results. It is with great pleasure that I can relay to you, on behalf of The United States Department Of Supernatural Distribution, that you are officially licensed to generation and application of banishment, disappearance, and-* The sturdy stationery and pompous print fade from view. Everything is temporarily blurry, and I feel my nose flare and quiver for a moment, before the rest of my body joins in. I\\'m not much of a crier, and make a pathetic scene for a moment as I let the emotion wash over me. Finally. Finally. Fucking finally. I blink the tears out of my eyes, and hear the soft thud\"\"\"\\n    \\n    Rewritten Text:\\n    \"\"\"My hand quivered as I clutched the letter, the parchment trembling beneath my grip. It was an official letter, bearing the seal of an agency I had never heard of – The United States Department Of Supernatural Distribution. The words danced across the page, promising me the power to dispel, vanish, and transport. My heart raced with anticipation, a cocktail of fear and exhilaration. The letter detailed my official licensing to generate and apply \"banishment, disappearance, and transmigration\" – powers hitherto unimaginable. The words \"banishment, disappearance, transmigration\" echoed in my mind like a siren\\'s song, promising me the ability to manipulate reality itself. The paper burned my eyes, and my nose twitched in anticipation of the intoxicating scent of power that I was about to wield. I read the code, my voice cracking with emotion, and the air crackled with anticipation. As I waved my hand and focused on the document, it vanished. It was as if I had conj<|endoftext|>'"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.decode(encoded_input[0])"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"data":{"text/plain":["'A new version of the original version of the book is available on Amazon.com/Amazon.com/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon/Amazon'"]},"execution_count":30,"metadata":{},"output_type":"execute_result"},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n","\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n","\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["decoded_output"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"data":{"text/plain":["'You are given 2 essays, the Rewritten text was created from the Original text using the google Gemma model.Analyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten text. Start directly with the prompt.\\n    \\n    Original Text:\\n    \"\"\"I can hardly read the letter, because the hand holding it is shaking so bad. My hand. There\\'s nothing wrong with me, I\\'m just nervous. I\\'m actually a pretty normal guy, painfully normal. But that\\'s all about to change. *Dear Mr. Thompson, we are quite happy to inform you that we have successfully received and processed your requests, as well as your official test results. It is with great pleasure that I can relay to you, on behalf of The United States Department Of Supernatural Distribution, that you are officially licensed to generation and application of banishment, disappearance, and-* The sturdy stationery and pompous print fade from view. Everything is temporarily blurry, and I feel my nose flare and quiver for a moment, before the rest of my body joins in. I\\'m not much of a crier, and make a pathetic scene for a moment as I let the emotion wash over me. Finally. Finally. Fucking finally. I blink the tears out of my eyes, and hear the soft thud\"\"\"\\n    \\n    Rewritten Text:\\n    \"\"\"My hand quivered as I clutched the letter, the parchment trembling beneath my grip. It was an official letter, bearing the seal of an agency I had never heard of – The United States Department Of Supernatural Distribution. The words danced across the page, promising me the power to dispel, vanish, and transport. My heart raced with anticipation, a cocktail of fear and exhilaration. The letter detailed my official licensing to generate and apply \"banishment, disappearance, and transmigration\" – powers hitherto unimaginable. The words \"banishment, disappearance, transmigration\" echoed in my mind like a siren\\'s song, promising me the ability to manipulate reality itself. The paper burned my eyes, and my nose twitched in anticipation of the intoxicating scent of power that I was about to wield. I read the code, my voice cracking with emotion, and the air crackled with anticipation. As I waved my hand and focused on the document, it vanished. It was as if I had conj'"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["\n","tokenizer.decode(encoded_input[0])"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"text/plain":["['<|endoftext|>']"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.all_special_tokens_extended"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"data":{"text/plain":["'ured up the magic of the word \"banishment, disappearance, and transmigration\" – powers hitherto unimaginable. The words \"banishment, disappearance, transmigration, and transmigration\" echoed in my mind like a siren\\'s song, promising me the power to manipulate reality itself. The paper burned my eyes, and my nose twitched in anticipation of the intoxicating scent of power that I was about to wield. I read the code, my voice cracking with emotion, and the air crackled with anticipation. The paper burned my eyes, and my nose twitched in anticipation of the intoxicating scent of power that I was about to'"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.decode(encoded_output[0]).replace(prompt, '')"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import time\n","time.sleep(100)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["try:\n","  del model\n","except: pass\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import logging\n","import os\n","os.makedirs('./logs/', exist_ok=True)\n","logging.basicConfig(\n","  level=logging.INFO,\n","  filename='./logs/log.txt', filemode='a',\n","  datefmt='%H:%M:%S',\n","  format='%(asctime)s - %(levelname)s - %(message)s',\n",")\n","\n","logger = logging.getLogger(__name__)\n","# Stream handler, logging to the stream\n","logger.info(\"abc\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:41:10.458119Z","iopub.status.busy":"2024-03-07T14:41:10.457747Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"]},{"name":"stdout","output_type":"stream","text":["GPT2LMHeadModel(\n","  (transformer): GPT2Model(\n","    (wte): Embedding(50257, 768)\n","    (wpe): Embedding(1024, 768)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0-11): 12 x GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Linear4bit(in_features=768, out_features=2304, bias=True)\n","          (c_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Linear4bit(in_features=768, out_features=3072, bias=True)\n","          (c_proj): Linear4bit(in_features=3072, out_features=768, bias=True)\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",")\n","trainable params: 589,824 || all params: 125,029,632 || trainable%: 0.4717473694555863\n","./working/trained_models/gpt2/checkpoint.pth\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5f583bb0a91c4cdeb5462f8c27f7c82b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["[2024-03-09 19:24:21,431] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"20016e0e42cd4f1f9a3b8a84347759fe","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d0978ad342604c6ba17af16cb49aca7e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import os\n","\n","batch_size = 3\n","grad_accumulation_steps = 64\n","num_epochs = 3\n","lr = 5e-5\n","checkpointing_steps = 500\n","save_path = os.path.join('./working/trained_models/', MODEL_PATH)\n","\n","# If ckpt_path is a real path (os.path.isfile(ckpt_path) is True),\n","# then the checkpoint will be loaded\n","ckpt_path = os.path.join(save_path, 'checkpoint.pth')\n","\n","args = (batch_size, num_epochs, lr, grad_accumulation_steps, \n","        checkpointing_steps, save_path, ckpt_path)\n","\n","if not os.path.exists(save_path):\n","    os.makedirs(save_path, exist_ok=True)\n","# notebook_launcher(main, args, num_processes=1)\n","main(*args)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["('./working/trained_models/gpt2/tokenizer_config.json',\n"," './working/trained_models/gpt2/special_tokens_map.json',\n"," './working/trained_models/gpt2/vocab.json',\n"," './working/trained_models/gpt2/merges.txt',\n"," './working/trained_models/gpt2/added_tokens.json',\n"," './working/trained_models/gpt2/tokenizer.json')"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.save_pretrained(save_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:39:45.155903Z","iopub.status.busy":"2024-03-07T14:39:45.155551Z","iopub.status.idle":"2024-03-07T14:39:45.160547Z","shell.execute_reply":"2024-03-07T14:39:45.159517Z","shell.execute_reply.started":"2024-03-07T14:39:45.155876Z"},"trusted":true},"outputs":[],"source":["try:\n","    del model, optimizer, train_dataloader, lr_scheduler\n","except Exception:\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:40:45.855872Z","iopub.status.busy":"2024-03-07T14:40:45.855499Z","iopub.status.idle":"2024-03-07T14:40:46.067760Z","shell.execute_reply":"2024-03-07T14:40:46.066318Z","shell.execute_reply.started":"2024-03-07T14:40:45.855842Z"},"trusted":true},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-07T14:37:47.408644Z","iopub.status.idle":"2024-03-07T14:37:47.408985Z","shell.execute_reply":"2024-03-07T14:37:47.408836Z","shell.execute_reply.started":"2024-03-07T14:37:47.408821Z"},"trusted":true},"outputs":[],"source":["import time\n","time.sleep(1000)"]},{"cell_type":"markdown","metadata":{},"source":["# Sub"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-07T14:37:47.410563Z","iopub.status.idle":"2024-03-07T14:37:47.410894Z","shell.execute_reply":"2024-03-07T14:37:47.410749Z","shell.execute_reply.started":"2024-03-07T14:37:47.410735Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","from tqdm import tqdm\n","\n","DEBUG = True\n","\n","TEST_DF_FILE = './input/llm-prompt-recovery/test.csv'\n","SUB_DF_FILE = './input/llm-prompt-recovery/sample_submission.csv'\n","NROWS = None if DEBUG else None\n","\n","if DEBUG:\n","    TEST_DF_FILE = './input/gemma-rewrite-nbroad/nbroad-v1.csv'\n","    SUB_DF_FILE = TEST_DF_FILE\n","\n","tdf = pd.read_csv(TEST_DF_FILE, nrows=NROWS, usecols=['id', 'original_text', 'rewritten_text'])\n","sub = pd.read_csv(SUB_DF_FILE, nrows=NROWS, usecols=['id', 'rewrite_prompt'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tdf = tdf.iloc[:2]\n","sub = sub.iloc[:2]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>original_text</th>\n","      <th>rewritten_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>LNpAovroGe</td>\n","      <td>This quilt, that my mother made, \\n \\n Still m...</td>\n","      <td>The softest brown and brightest blue quilt, cr...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>nnuxwwThWi</td>\n","      <td>It's the job of our agency to keep track of th...</td>\n","      <td>The agency's responsibility is to track and co...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           id                                      original_text  \\\n","0  LNpAovroGe  This quilt, that my mother made, \\n \\n Still m...   \n","1  nnuxwwThWi  It's the job of our agency to keep track of th...   \n","\n","                                      rewritten_text  \n","0  The softest brown and brightest blue quilt, cr...  \n","1  The agency's responsibility is to track and co...  "]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["tdf"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>rewrite_prompt</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>LNpAovroGe</td>\n","      <td>Regency Romance: Model the text on a Regency r...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>nnuxwwThWi</td>\n","      <td>Write like Ernest Hemingway: Focus on Hemingwa...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           id                                     rewrite_prompt\n","0  LNpAovroGe  Regency Romance: Model the text on a Regency r...\n","1  nnuxwwThWi  Write like Ernest Hemingway: Focus on Hemingwa..."]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["sub"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-07T14:37:47.412789Z","iopub.status.idle":"2024-03-07T14:37:47.413261Z","shell.execute_reply":"2024-03-07T14:37:47.413020Z","shell.execute_reply.started":"2024-03-07T14:37:47.413002Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/2 [00:00<?, ?it/s]`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","\n","No chat template is defined for this tokenizer - using the default template for the GPT2TokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n","\n","A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"]},{"name":"stdout","output_type":"stream","text":["Model of epoch 2 is loaded\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 1/2 [00:02<00:02,  2.59s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","100%|██████████| 2/2 [00:03<00:00,  1.69s/it]\n"]}],"source":["import gc\n","import re\n","\n","device = 'cuda'\n","tdf['id'] = sub['id'].copy()\n","\n","pbar = tqdm(total=tdf.shape[0])\n","\n","it = iter(tdf.iterrows())\n","idx, row = next(it, (None, None))\n","\n","# https://www.kaggle.com/competitions/llm-prompt-recovery/discussion/481116\n","DEFAULT_TEXT = 'Please improve the following text, maintaining the original meaning but altering the tone, diction, and stylistic elements to match the new style.Enhance the clarity, elegance, and impact of the following text by adopting the writing style of , ensuring the core message remains intact while transforming the tone, word choice, and stylistic features to align with the specified style.' \n","\n","res = []\n","model = None\n","while idx is not None:\n","    \n","    if (datetime.datetime.now() - start_time) > datetime.timedelta(hours=8, minutes=30):\n","        res.append([row[\"id\"], DEFAULT_TEXT])\n","        idx, row = next(it, (None, None))\n","        pbar.update(1)\n","        continue\n","        \n","    torch.cuda.empty_cache()\n","    gc.collect()\n","        \n","    try:\n","        if model is None:\n","            ckpt_path = './working/trained_models/%s/checkpoint.pth' % MODEL_PATH\n","            quantization_config = BitsAndBytesConfig(\n","                load_in_4bit=True, \n","                bnb_4bit_quant_type=\"nf4\",\n","                bnb_4bit_compute_dtype=torch.bfloat16,\n","                bnb_4bit_use_double_quant=True,\n","            )\n","            model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, \n","                                                         quantization_config=quantization_config, \n","                                                         torch_dtype=torch.float16)\n","            checkpoint = torch.load(ckpt_path)\n","\n","            model.load_state_dict(checkpoint['model'], strict=False)\n","            print(f\"Model of epoch {checkpoint['epoch']} is loaded\")\n","        \n","        \n","        model.eval()    \n","            \n","            \n","        messages = [\n","            {\n","                \"role\": \"user\",\n","                \"content\": gen_prompt(row[\"original_text\"], row[\"rewritten_text\"])\n","            }\n","        ]\n","        encoded_input = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(device)\n","        \n","        with torch.no_grad():\n","            encoded_output = model.generate(encoded_input, max_new_tokens=50, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n","        \n","        decoded_output = tokenizer.batch_decode(encoded_output, skip_special_tokens=True)[0]\n","        decoded_output = result = re.sub(r\"[\\s\\S]*\\[\\/INST\\]\", '', decoded_output, 1)\n","                \n","        res.append([row[\"id\"], decoded_output])\n","                            \n","    except Exception as e:\n","        print(f\"ERROR: {e}\")\n","        res.append([row[\"id\"], DEFAULT_TEXT])\n","        \n","    finally:\n","        idx, row = next(it, (None, None))\n","        pbar.update(1)\n","\n","        \n","pbar.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-07T14:37:47.414902Z","iopub.status.idle":"2024-03-07T14:37:47.415620Z","shell.execute_reply":"2024-03-07T14:37:47.415397Z","shell.execute_reply.started":"2024-03-07T14:37:47.415377Z"},"trusted":true},"outputs":[],"source":["# sub[\"rewrite_prompt\"] = tdf['rewrite_prompt'].copy()\n","# sub.to_csv(\"submission.csv\", index=False)\n","sub = pd.DataFrame(res, columns=['id', 'rewrite_prompt'])\n","\n","sub.to_csv(\"sample_submission.csv\", index=False)\n","sub.to_csv(\"submission.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-07T14:37:47.416641Z","iopub.status.idle":"2024-03-07T14:37:47.417090Z","shell.execute_reply":"2024-03-07T14:37:47.416867Z","shell.execute_reply.started":"2024-03-07T14:37:47.416848Z"},"trusted":true},"outputs":[],"source":["sub"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-07T14:37:47.419040Z","iopub.status.idle":"2024-03-07T14:37:47.419499Z","shell.execute_reply":"2024-03-07T14:37:47.419288Z","shell.execute_reply.started":"2024-03-07T14:37:47.419269Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[['LNpAovroGe',\n","  'Original Text:\\n    \"\"\"This quilt, that my mother made, \\n \\n Still makes me think to this day. \\n \\n It\\'s softest brown, and brightest blue, \\n \\n The curved stitch here, reads `` made it May\\'\\'. \\n \\n It\\'s hard to see, but believe me it\\'s true, \\n \\n That\\'s not just a cloth but a piece of shirt. \\n \\n You can see a logo here, and right there, \\n \\n And a signature over there, someone named `` Bert\\'\\'. \\n \\n This is my favorite part, a piece from a stuffed bear. \\n \\n I think it was my mother\\'s favorite too, \\n \\n She always said so at least. \\n \\n Something from when she was two, \\n \\n Given by her grandad for Thanksgiving feast. \\n \\n My dad added this, a little button pin, \\n \\n Something from his mother, for being a scout. \\n \\n Apparently she went to a store and fished in a bin, \\n \\n Until night that day, to teach him what love was about. \\n \\n I\\'m sorry you had to see this, \\n \\n but their funeral was delayed. \\n \\n \\n \\n\\n\"\"\"\\n    \\n    Rewritten Text:\\n    \"\"\"The softest brown and brightest blue quilt, crafted by the loving hand of a mother, evokes vivid memories in my mind. The curved stitch, delicately etched upon its surface, bears the inscription \"made it May,\" a testament to the passage of time. Though the fabric may be veiled in mystery, I firmly believe that this quilt is not merely a cloth but a treasured heirloom, imbued with love and sentiment. The intricate logo and signature, meticulously woven into the quilt\\'s fabric, reveal the name \"Bert,\" a symbol of the owner\\'s identity. This precious artifact, once a part of a stuffed bear, holds a special place in my heart. It was my mother\\'s favorite, a cherished relic from her youth, gifted to her by her grandfather during a Thanksgiving feast. My father added his own touch to the quilt, a pinned button that symbolized his mother\\'s unwavering spirit and her dedication to scouting. Apparently, she spent the entire night fishing in a store bin, determined to teach her son about the\"\"\"\\n    \\n    You are given 2 essays, the Rewritten text was created from the Original text using the google Gemma model.\\n    Analyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten text.\\n    Start directly with the prompt, output should be one line only.A young woman went on a five day journey when she met her father on a night out to see what was going on with her life before the death. The person who met with her father to get her out of hiding and try to get her out'],\n"," ['nnuxwwThWi',\n","  'Original Text:\\n    \"\"\"It\\'s the job of our agency to keep track of the worlds monsters and keep them locked down. Which is complicated because people aren\\'t allowed to know they exist or that they create them. Fear itself, when amplified by a large amount of people, fearing that same thing enough... produces a monster to represent it, it simply comes to life and exists. We stop said Monsters from destroying us. We always do. This time just over a decade ago, our job got a lot harder, a new breed of monster had appeared, and it was more dangerous than we ever suspected possible. The thing about the old monsters is they\\'re like the fear they represent: Werewolves were simply animals, animals that stick to the forest, they\\'re easy to explain to locals and generally don\\'t even cause much trouble so long as they\\'re left well enough alone, we picked off the aggressive ones and left the rest alone. Vampires were a problem for a long while, because like the\"\"\"\\n    \\n    Rewritten Text:\\n    \"\"\"The agency\\'s responsibility is to track and contain the monstrous creatures that roam the world. This task proves challenging due to the taboo surrounding their existence and the fear they induce. Fear, amplified by a large number of individuals, breeds a monstrous representation of that fear. We engage in combat against these creatures, ensuring their destruction. Recently, a decade ago, our work became infinitely more difficult. A new breed of monster emerged, one more menacing than any we had encountered previously. Unlike the old monsters, these creatures are like the fear they represent: animals that embody the wildness of nature, vampires that exploit their power to prey upon others, and zombies whose existence is rooted in the fear of disease. However, our latest adversary presents a unique challenge. Unlike the old monsters, which are relatively straightforward to identify and defeat, these creatures self-destruct mere seconds after their creation. Their existence is fleeting, yet their destructive force is undeniable. It is this new monster that has us grappling with the complexities\"\"\"\\n    \\n    You are given 2 essays, the Rewritten text was created from the Original text using the google Gemma model.\\n    Analyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten text.\\n    Start directly with the prompt, output should be one line only.Cities and suburbs all hate their own cities and are in the grips of a rising tide of \"populist madness that threatens to eclipse and even destroy all that\\'s left.\"\\n\\nThe movement, called \"populism to save cities and']]"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["res"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":7806901,"sourceId":67121,"sourceType":"competition"},{"datasetId":4506214,"sourceId":7747717,"sourceType":"datasetVersion"},{"sourceId":140952720,"sourceType":"kernelVersion"},{"sourceId":164836055,"sourceType":"kernelVersion"},{"modelInstanceId":284,"sourceId":386,"sourceType":"modelInstanceVersion"},{"modelInstanceId":3900,"sourceId":5112,"sourceType":"modelInstanceVersion"},{"modelInstanceId":4761,"sourceId":5994,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelInstanceId":8332,"sourceId":11394,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
