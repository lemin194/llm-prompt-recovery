{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-03-07T14:34:34.653535Z","iopub.status.busy":"2024-03-07T14:34:34.652850Z","iopub.status.idle":"2024-03-07T14:34:40.781469Z","shell.execute_reply":"2024-03-07T14:34:40.780703Z","shell.execute_reply.started":"2024-03-07T14:34:34.653491Z"},"trusted":true},"outputs":[],"source":["from accelerate.utils import BnbQuantizationConfig\n","from accelerate import Accelerator, notebook_launcher\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, \\\n","                        get_cosine_schedule_with_warmup, set_seed\n","import transformers\n","import optimum\n","\n","from datasets import load_dataset,Dataset\n","import pandas as pd\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, \\\n","                        get_cosine_schedule_with_warmup, set_seed\n","from peft import LoraConfig, TaskType, get_peft_model\n","from accelerate import Accelerator, notebook_launcher\n","from accelerate.utils import set_seed\n","import logging\n","from torch.utils.data import DataLoader\n","from torch.optim import AdamW, SGD\n","from tqdm.notebook import tqdm\n","import torch\n","from torch.nn.utils.rnn import pad_sequence\n","import glob\n","from collections import OrderedDict\n","import re\n","\n","import os"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:34:40.783110Z","iopub.status.busy":"2024-03-07T14:34:40.782634Z","iopub.status.idle":"2024-03-07T14:34:40.787187Z","shell.execute_reply":"2024-03-07T14:34:40.786265Z","shell.execute_reply.started":"2024-03-07T14:34:40.783080Z"},"trusted":true},"outputs":[],"source":["import datetime\n","start_time = datetime.datetime.now()"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:34:40.790460Z","iopub.status.busy":"2024-03-07T14:34:40.790109Z","iopub.status.idle":"2024-03-07T14:34:40.800232Z","shell.execute_reply":"2024-03-07T14:34:40.799424Z","shell.execute_reply.started":"2024-03-07T14:34:40.790431Z"},"trusted":true},"outputs":[],"source":["def truncate_txt(text, length):\n","    text_list = text.split()\n","    \n","    if len(text_list) <= length:\n","        return text\n","    \n","    return \" \".join(text_list[:length])\n","\n","\n","def gen_prompt(og_text, rewritten_text):\n","    \n","    # Truncate the texts to first 200 words for now\n","    # As we are having memory issues on Mixtral8x7b\n","    og_text = truncate_txt(og_text, 170)\n","    rewritten_text = truncate_txt(rewritten_text, 170)\n","    \n","    return f\"\"\"    \n","    Original Text:\n","    \\\"\"\"{og_text}\\\"\"\"\n","    \n","    Rewritten Text:\n","    \\\"\"\"{rewritten_text}\\\"\"\"\n","    \n","    You are given 2 essays, the Rewritten text was created from the Original text using the google Gemma model.\n","    Analyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten text.\n","    Start directly with the prompt, output should be one line only.\n","    \"\"\".strip()"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:34:40.803388Z","iopub.status.busy":"2024-03-07T14:34:40.803090Z","iopub.status.idle":"2024-03-07T14:34:42.359073Z","shell.execute_reply":"2024-03-07T14:34:42.358255Z","shell.execute_reply.started":"2024-03-07T14:34:40.803366Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","filename1 = \"./input/gemma-rewrite-nbroad/nbroad-v1.csv\"\n","filename2 = \"./input/gemma-rewrite-nbroad/nbroad-v2.csv\"\n","df = pd.concat([pd.read_csv(filename1), pd.read_csv(filename2)]).reset_index(drop=True)\n","\n","df['reverse_prompt'] = df.apply(lambda x: gen_prompt(x.original_text, x.rewritten_text), axis=1)\n","df = df.iloc[:6]\n","\n","data = Dataset.from_pandas(df[['reverse_prompt', 'rewrite_prompt']],split='train')"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:34:42.360542Z","iopub.status.busy":"2024-03-07T14:34:42.360264Z","iopub.status.idle":"2024-03-07T14:34:46.357229Z","shell.execute_reply":"2024-03-07T14:34:46.356439Z","shell.execute_reply.started":"2024-03-07T14:34:42.360518Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8f137a7112a94db086411d42cdc81c24","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/6 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# MODEL_PATH = \"/kaggle/input/gemma/transformers/7b-it/2\"\n","MODEL_PATH = \"distilbert/distilgpt2\"\n","# MODEL_PATH = \"gpt2\"\n","# MODEL_PATH = \"distilbert/distilroberta-base\"\n","# MODEL_PATH = 'mistralai/Mistral-7B-Instruct-v0.2'\n","# MODEL_PATH = \"google/gemma-2b-it\"\n","# MODEL_PATH = \"google/flan-t5-small\"\n","# MODEL_PATH = \"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\"\n","# MODEL_PATH = \"/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1\"\n","# MODEL_PATH = \"/kaggle/input/llama-2/pytorch/7b-chat-hf/1\"\n","# MODEL_PATH = \"/kaggle/input/llama-2/pytorch/13b-chat-hf/1\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n","def tokenize_samples(samples):\n","    inputs = tokenizer(samples[\"reverse_prompt\"], max_length=512, truncation=True)\n","    targets = tokenizer(samples[\"rewrite_prompt\"], max_length=512, truncation=True)\n","    return {'input_ids': inputs['input_ids'], 'attention_mask': inputs['attention_mask'], 'labels': targets['input_ids']}\n","    \n","data = data.map(tokenize_samples, batched=True)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["('Regency Romance: Model the text on a Regency romance novel, focusing on social gatherings, romantic pursuits, and the strict manners of the era.',\n"," 'Regency Romance: Model the text on a Regency romance novel, focusing on social gatherings, romantic pursuits, and the strict manners of the era.')"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["encoded = tokenize_samples(df.iloc[0])\n","tokenizer.decode(encoded['labels']), df.iloc[0].rewrite_prompt"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:34:46.358636Z","iopub.status.busy":"2024-03-07T14:34:46.358351Z","iopub.status.idle":"2024-03-07T14:34:46.367556Z","shell.execute_reply":"2024-03-07T14:34:46.366644Z","shell.execute_reply.started":"2024-03-07T14:34:46.358612Z"},"trusted":true},"outputs":[],"source":["def collate_fn(batch):\n","    inputs = [torch.tensor(b['input_ids']) for b in batch]\n","    labels = [torch.tensor(b['labels']) for b in batch]\n","\n","    max_length = max(len(input_) for input_ in inputs)\n","\n","    input_ids = pad_sequence(inputs, batch_first=True, padding_value=tokenizer.pad_token_id)\n","\n","    padded_labels = []\n","    for label in labels:\n","        padded_label = torch.full((max_length,), fill_value=-100, dtype=torch.long)\n","        padded_label[:len(label)] = label\n","        padded_labels.append(padded_label)\n","    padded_labels = torch.stack(padded_labels)\n","\n","    attention_mask = torch.ones(input_ids.shape, dtype=torch.long)\n","    attention_mask[input_ids == tokenizer.pad_token_id] = 0\n","\n","    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': padded_labels}\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:34:46.368907Z","iopub.status.busy":"2024-03-07T14:34:46.368641Z","iopub.status.idle":"2024-03-07T14:34:46.378598Z","shell.execute_reply":"2024-03-07T14:34:46.377738Z","shell.execute_reply.started":"2024-03-07T14:34:46.368881Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<function collate_fn at 0x76851d8a3370>\n"]}],"source":["print(collate_fn)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:34:46.379858Z","iopub.status.busy":"2024-03-07T14:34:46.379580Z","iopub.status.idle":"2024-03-07T14:34:46.388744Z","shell.execute_reply":"2024-03-07T14:34:46.387945Z","shell.execute_reply.started":"2024-03-07T14:34:46.379832Z"},"trusted":true},"outputs":[],"source":["# import torch\n","# from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig\n","\n","\n","# # Found a good blog to catch me up fast!\n","# # https://huggingface.co/blog/4bit-transformers-bitsandbytes\n","# # https://huggingface.co/docs/transformers/v4.38.1/en/quantization#compute-data-type\n","# quantization_config = BitsAndBytesConfig(\n","#     load_in_4bit = True,\n","#     bnb_4bit_quant_type=\"nf4\",\n","#     bnb_4bit_compute_dtype=torch.bfloat16,\n","#     bnb_4bit_use_double_quant=True,\n","# )\n","\n","\n","\n","# model = AutoModelForCausalLM.from_pretrained(\n","#     MODEL_PATH,\n","#     device_map = \"auto\",\n","#     trust_remote_code = True,\n","#     quantization_config=quantization_config,\n","# )\n","\n","# # model = model.to_bettertransformer()\n","# model = accelerator.prepare(model)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:34:46.403559Z","iopub.status.busy":"2024-03-07T14:34:46.403334Z","iopub.status.idle":"2024-03-07T14:34:46.415815Z","shell.execute_reply":"2024-03-07T14:34:46.414865Z","shell.execute_reply.started":"2024-03-07T14:34:46.403539Z"},"trusted":true},"outputs":[],"source":["def del_past_models(save_path, file_exten='pth'):\n","    \"\"\"\n","    Remove all of the past models\n","    \n","    You can change the file_extension if you save the models with other file_extension\n","    \"\"\"\n","    past_models = glob.glob(os.path.join(save_path, '*.' + file_exten))\n","    for past_model in past_models:\n","        os.remove(past_model)\n","        logger.info(f'Remove model {past_model}!')\n","        \n","def save_checkpoint(path, model, optim, sched, epoch, iters):\n","    lr = optim.param_groups[0]['lr']\n","    # model_state = model.state_dict()\n","    model_state = OrderedDict((name, param) for name, param in model.named_parameters() \\\n","                    if param.requires_grad)\n","    \n","    checkpoint_path = os.path.join(path, f'checkpoint_{iters + 1}.pth')\n","    new_checkpoint_path = os.path.join(path, f'checkpoint.pth')\n","    logger.info(f\"Model of epoch {epoch} saved at checkpoint_{iters + 1}.pth, lr={lr:.3e}\")\n","    \n","    torch.save({\n","        'model': model_state,\n","        'optimizer': optim.state_dict(),\n","        'scheduler': sched.state_dict(), \n","        'epoch': epoch\n","    }, checkpoint_path)\n","    \n","    torch.save({\n","        'model': model_state,\n","        'optimizer': optim.state_dict(),\n","        'scheduler': sched.state_dict(), \n","        'epoch': epoch\n","    }, new_checkpoint_path)\n","\n","    \n","def load_checkpoint(checkpoint_path, model, optim=None, sched=None):\n","    checkpoint = torch.load(checkpoint_path)\n","    logger.info(f\"Model of epoch {checkpoint['epoch']} is loaded\")\n","    \n","    model.load_state_dict(checkpoint['model'], strict=False)\n","    if optim is not None and sched is not None:\n","        optim.load_state_dict(checkpoint['optimizer'])\n","        sched.load_state_dict(checkpoint['scheduler'])\n","        return model, optim, sched, checkpoint['epoch']\n","    else:\n","        return model, checkpoint['epoch']"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:41:09.200753Z","iopub.status.busy":"2024-03-07T14:41:09.200368Z","iopub.status.idle":"2024-03-07T14:41:09.223448Z","shell.execute_reply":"2024-03-07T14:41:09.222511Z","shell.execute_reply.started":"2024-03-07T14:41:09.200725Z"},"trusted":true},"outputs":[],"source":["def train_epoch(epoch, model, accelerator, \n","                train_dataloader, checkpointing_steps, \n","                optimizer, lr_scheduler, save_path):\n","    global overall_step\n","    model.train()\n","    epoch_loss = []\n","    pbar = tqdm(train_dataloader)\n","    grad = torch.tensor(0.0)\n","    \n","    output_dir = f\"step_{overall_step}.pth\"\n","    unwrapped_model = accelerator.unwrap_model(model)\n","    save_checkpoint(save_path, unwrapped_model, optimizer, lr_scheduler, \n","                   epoch, overall_step)\n","    \n","    for step, batch in enumerate(pbar):\n","        if (datetime.datetime.now() - start_time) > datetime.timedelta(hours=6):\n","            break\n","        with accelerator.accumulate(model):\n","            # Gradient accumulation\n","            # with accelerator.autocast():\n","            outputs = model(**batch)\n","            loss = outputs.loss\n","            accelerator.backward(loss)\n","            if accelerator.sync_gradients:\n","                grad = accelerator.clip_grad_norm_(parameters=model.parameters(), \n","                                                   max_norm=2.0)\n","            optimizer.step()\n","            lr_scheduler.step()\n","            optimizer.zero_grad()\n","\n","        overall_step += 1\n","        lr = optimizer.param_groups[0]['lr']\n","        \n","        pbar.set_description(\n","            f'Epoch {epoch}: loss = {loss.item(): .3f}, grad = {grad.item(): .3f}, lr = {lr: .3e}')\n","        \n","        with torch.no_grad():\n","            avg_loss = accelerator.gather(loss.repeat(len(batch))).mean()\n","        epoch_loss.append(avg_loss.item() / accelerator.gradient_accumulation_steps)\n","        \n","        accelerator.wait_for_everyone()\n","        if overall_step % checkpointing_steps == 0:\n","            logger.info(\n","                f'Epoch {epoch}: loss = {loss.item(): .3f}, grad = {grad.item(): .3f}, lr = {lr: .3e}')\n","            if accelerator.is_local_main_process:\n","                # Clear all of the current models\n","                del_past_models(save_path)\n","\n","                output_dir = f\"step_{overall_step}.pth\"\n","                unwrapped_model = accelerator.unwrap_model(model)\n","                save_checkpoint(save_path, unwrapped_model, optimizer, lr_scheduler, \n","                               epoch, overall_step)\n","    \n","    # Just log the loss of the main process\n","    if len(epoch_loss) > 0:\n","        logger.info(f'Epoch {epoch}: loss = {sum(epoch_loss) / (len(epoch_loss)+1e-10): .3f}, lr = {lr: .3e}')\n","    \n","            \n","def main(batch_size: int, num_epochs: int, lr: float, grad_accumulation_steps: int, \n","         checkpointing_steps: int, save_path: str, ckpt_path: str,\n","         num_warmup_steps: int=0, r: int=4, lora_alpha: int=32, lora_dropout: float=0.1):\n","    set_seed(1234)\n","    \n","    accelerator = Accelerator(gradient_accumulation_steps=grad_accumulation_steps)\n","    quantization_config = BitsAndBytesConfig(\n","        load_in_4bit=True, \n","        # bnb_4bit_quant_type=\"nf4\",\n","        bnb_4bit_compute_dtype=torch.bfloat16,\n","        # bnb_4bit_use_double_quant=True,\n","    )\n","    \n","    model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, \n","                                                 quantization_config=quantization_config, \n","                                                 torch_dtype=torch.float16)\n","    model.save_pretrained(save_path)\n","    tokenizer.save_pretrained(save_path)\n","    \n","    \n","    accelerator.print(model)\n","    \n","    # Instantiate dataloaders. (We do not split the test data)\n","    train_dataloader = DataLoader(\n","        data, shuffle=True, collate_fn=collate_fn, batch_size=batch_size\n","    )\n","    \n","    peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, \n","                             inference_mode=False, r=r, \n","                             lora_alpha=lora_alpha, lora_dropout=lora_dropout,\n","                             target_modules=\n","                            lora_target_modules_dict.get(MODEL_PATH, [\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],)\n","                            )\n","    model = get_peft_model(model, peft_config)\n","    \n","    if accelerator.is_local_main_process:\n","        model.print_trainable_parameters()\n","    \n","    lr = lr * accelerator.num_processes * accelerator.gradient_accumulation_steps\n","    \n","    optimizer = AdamW(params=model.parameters(), lr=lr)\n","    \n","    total_steps = len(train_dataloader) * num_epochs * \\\n","            accelerator.num_processes * accelerator.gradient_accumulation_steps\n","    \n","    # Instantiate scheduler\n","    lr_scheduler = get_cosine_schedule_with_warmup(\n","        optimizer=optimizer,\n","        num_warmup_steps=num_warmup_steps,\n","        # to ensure the lr will not become zero at the end of training\n","        # (You can adjust this param)\n","        num_training_steps=total_steps * 1.1,\n","    )\n","    \n","    current_epochs = 0\n","    global overall_step\n","    overall_step = 0\n","    \n","    # Load checkpoint\n","    print(ckpt_path)\n","    if os.path.isfile(ckpt_path):\n","        print('loading')\n","        model, optimizer, lr_scheduler, current_epochs = \\\n","                    load_checkpoint(ckpt_path, model, optimizer, lr_scheduler)\n","        c = re.search('(\\d)+', ckpt_path)\n","        if c is None: overall_step = 1\n","        else: overall_step = int(c.group())\n","        logger.info(\n","            f'Checkpoint {ckpt_path} loaded at epoch {current_epochs}, the training will resume from epoch {current_epochs + 1}!')\n","        current_epochs += 1\n","    \n","    \n","    if current_epochs >= num_epochs:\n","        raise ValueError('The num_epochs should be larger than the saved epochs!!')\n","    \n","    # Prepare everything\n","    # There is no specific order to remember, \n","    # we just need to unpack the objects in the same order we gave them to the prepare method.\n","    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n","        model, optimizer, train_dataloader, lr_scheduler\n","    )\n","    \n","    \n","    logger.info('*********************** Start training! **************************')\n","    \n","    for epoch in range(current_epochs, num_epochs):\n","        train_epoch(epoch, model, accelerator, train_dataloader, \n","                    checkpointing_steps, optimizer, lr_scheduler, save_path)\n","        \n","    # Save the final model\n","    accelerator.wait_for_everyone()\n","    if accelerator.is_local_main_process:\n","        output_dir = f\"step_{overall_step}.pth\"\n","        unwrapped_model = accelerator.unwrap_model(model)\n","        save_checkpoint(save_path, unwrapped_model, optimizer, lr_scheduler, \n","                       epoch, overall_step)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["lora_target_modules_dict = {\n","  'gpt2': ['c_attn'],\n","  'distilbert/distilgpt2': ['c_attn'],\n","  'distilbert/distilroberta-base': ['query', 'key', 'value'],\n","}\n","import json\n","os.makedirs('./settings', exist_ok=True)\n","json.dump(lora_target_modules_dict, open('./settings/lora_target_modules.json', 'w'))"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:34:46.442375Z","iopub.status.busy":"2024-03-07T14:34:46.442144Z","iopub.status.idle":"2024-03-07T14:34:46.455275Z","shell.execute_reply":"2024-03-07T14:34:46.454417Z","shell.execute_reply.started":"2024-03-07T14:34:46.442355Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["50256\n"]}],"source":["if tokenizer.pad_token_id is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","print(tokenizer.pad_token_id)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["import logging\n","import os\n","import datetime\n","os.makedirs('./logs/', exist_ok=True)\n","logging.basicConfig(\n","  level=logging.INFO,\n","  filename='./logs/log_%s.txt' % datetime.datetime.now().strftime('%y%m%d%H%m%S'), filemode='a',\n","  datefmt='%H:%M:%S',\n","  format='%(asctime)s - %(levelname)s - %(message)s',\n",")\n","\n","logger = logging.getLogger(__name__)\n","# Stream handler, logging to the stream\n","logger.info(\"abc\")"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Jupyter Capture Output v0.0.11\n"]}],"source":["import jupyter_capture_output"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:41:10.458119Z","iopub.status.busy":"2024-03-07T14:41:10.457747Z"},"trusted":true},"outputs":[],"source":["%%capture_text --path \"cap/overfitting.txt\"\n","\n","import os\n","\n","batch_size = 2\n","grad_accumulation_steps = 1\n","num_epochs = 600\n","lr = 5e-5\n","checkpointing_steps = 500\n","save_path = os.path.join('./working/trained_models/', MODEL_PATH)\n","r = 64\n","lora_alpha = 64\n","lora_dropout = 0.05\n","\n","# If ckpt_path is a real path (os.path.isfile(ckpt_path) is True),\n","# then the checkpoint will be loaded\n","ckpt_path = os.path.join(save_path, 'checkpoint.pth')\n","print(ckpt_path)\n","kwargs = {\n","'batch_size':batch_size, 'num_epochs':num_epochs, 'lr':lr, 'grad_accumulation_steps':grad_accumulation_steps, \n","'checkpointing_steps':checkpointing_steps, 'save_path':save_path, 'ckpt_path':ckpt_path, 'r':r, 'lora_alpha':lora_alpha, 'lora_dropout':lora_dropout\n","}\n","\n","if not os.path.exists(save_path):\n","    os.makedirs(save_path, exist_ok=True)\n","# notebook_launcher(main, args, num_processes=1)\n","main(**kwargs)"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":7806901,"sourceId":67121,"sourceType":"competition"},{"datasetId":4506214,"sourceId":7747717,"sourceType":"datasetVersion"},{"sourceId":140952720,"sourceType":"kernelVersion"},{"sourceId":164836055,"sourceType":"kernelVersion"},{"modelInstanceId":284,"sourceId":386,"sourceType":"modelInstanceVersion"},{"modelInstanceId":3900,"sourceId":5112,"sourceType":"modelInstanceVersion"},{"modelInstanceId":4761,"sourceId":5994,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelInstanceId":8332,"sourceId":11394,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
