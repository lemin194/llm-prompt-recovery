{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-03-07T14:34:34.653535Z","iopub.status.busy":"2024-03-07T14:34:34.652850Z","iopub.status.idle":"2024-03-07T14:34:40.781469Z","shell.execute_reply":"2024-03-07T14:34:40.780703Z","shell.execute_reply.started":"2024-03-07T14:34:34.653491Z"},"trusted":true},"outputs":[],"source":["from accelerate.utils import BnbQuantizationConfig\n","from accelerate import Accelerator, notebook_launcher\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, \\\n","                        get_cosine_schedule_with_warmup, set_seed\n","import transformers\n","import optimum\n","\n","from datasets import load_dataset,Dataset\n","import pandas as pd\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, \\\n","                        get_cosine_schedule_with_warmup, set_seed\n","from peft import LoraConfig, TaskType, get_peft_model\n","from accelerate import Accelerator, notebook_launcher\n","from accelerate.utils import set_seed\n","import logging\n","from torch.utils.data import DataLoader\n","from torch.optim import AdamW, SGD\n","from tqdm.notebook import tqdm\n","import torch\n","from torch.nn.utils.rnn import pad_sequence\n","import glob\n","from collections import OrderedDict\n","import re\n","\n","import os"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:34:40.783110Z","iopub.status.busy":"2024-03-07T14:34:40.782634Z","iopub.status.idle":"2024-03-07T14:34:40.787187Z","shell.execute_reply":"2024-03-07T14:34:40.786265Z","shell.execute_reply.started":"2024-03-07T14:34:40.783080Z"},"trusted":true},"outputs":[],"source":["import datetime\n","start_time = datetime.datetime.now()"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:34:40.790460Z","iopub.status.busy":"2024-03-07T14:34:40.790109Z","iopub.status.idle":"2024-03-07T14:34:40.800232Z","shell.execute_reply":"2024-03-07T14:34:40.799424Z","shell.execute_reply.started":"2024-03-07T14:34:40.790431Z"},"trusted":true},"outputs":[],"source":["def truncate_txt(text, length):\n","    text_list = text.split()\n","    \n","    if len(text_list) <= length:\n","        return text\n","    \n","    return \" \".join(text_list[:length])\n","\n","\n","def gen_prompt(og_text, rewritten_text):\n","    \n","    # Truncate the texts to first 200 words for now\n","    # As we are having memory issues on Mixtral8x7b\n","    og_text = truncate_txt(og_text, 170)\n","    rewritten_text = truncate_txt(rewritten_text, 170)\n","    \n","    return f\"\"\"    \n","    Original Text:\n","    \\\"\"\"{og_text}\\\"\"\"\n","    \n","    Rewritten Text:\n","    \\\"\"\"{rewritten_text}\\\"\"\"\n","    \n","    You are given 2 essays, the Rewritten text was created from the Original text using the google Gemma model.\n","    Analyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten text.\n","    Start directly with the prompt, output should be one line only.\n","    \"\"\".strip()"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:34:40.803388Z","iopub.status.busy":"2024-03-07T14:34:40.803090Z","iopub.status.idle":"2024-03-07T14:34:42.359073Z","shell.execute_reply":"2024-03-07T14:34:42.358255Z","shell.execute_reply.started":"2024-03-07T14:34:40.803366Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","filename1 = \"./input/gemma-rewrite-nbroad/nbroad-v1.csv\"\n","filename2 = \"./input/gemma-rewrite-nbroad/nbroad-v2.csv\"\n","df = pd.concat([pd.read_csv(filename1), pd.read_csv(filename2)]).reset_index(drop=True)\n","\n","df['reverse_prompt'] = df.apply(lambda x: gen_prompt(x.original_text, x.rewritten_text), axis=1)\n","df = df.iloc[:6]\n","\n","data = Dataset.from_pandas(df[['reverse_prompt', 'rewrite_prompt']],split='train')"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:34:42.360542Z","iopub.status.busy":"2024-03-07T14:34:42.360264Z","iopub.status.idle":"2024-03-07T14:34:46.357229Z","shell.execute_reply":"2024-03-07T14:34:46.356439Z","shell.execute_reply.started":"2024-03-07T14:34:42.360518Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8f137a7112a94db086411d42cdc81c24","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/6 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# MODEL_PATH = \"/kaggle/input/gemma/transformers/7b-it/2\"\n","MODEL_PATH = \"distilbert/distilgpt2\"\n","# MODEL_PATH = \"gpt2\"\n","# MODEL_PATH = \"distilbert/distilroberta-base\"\n","# MODEL_PATH = 'mistralai/Mistral-7B-Instruct-v0.2'\n","# MODEL_PATH = \"google/gemma-2b-it\"\n","# MODEL_PATH = \"google/flan-t5-small\"\n","# MODEL_PATH = \"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\"\n","# MODEL_PATH = \"/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1\"\n","# MODEL_PATH = \"/kaggle/input/llama-2/pytorch/7b-chat-hf/1\"\n","# MODEL_PATH = \"/kaggle/input/llama-2/pytorch/13b-chat-hf/1\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n","def tokenize_samples(samples):\n","    inputs = tokenizer(samples[\"reverse_prompt\"], max_length=512, truncation=True)\n","    targets = tokenizer(samples[\"rewrite_prompt\"], max_length=512, truncation=True)\n","    return {'input_ids': inputs['input_ids'], 'attention_mask': inputs['attention_mask'], 'labels': targets['input_ids']}\n","    \n","data = data.map(tokenize_samples, batched=True)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["('Regency Romance: Model the text on a Regency romance novel, focusing on social gatherings, romantic pursuits, and the strict manners of the era.',\n"," 'Regency Romance: Model the text on a Regency romance novel, focusing on social gatherings, romantic pursuits, and the strict manners of the era.')"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["encoded = tokenize_samples(df.iloc[0])\n","tokenizer.decode(encoded['labels']), df.iloc[0].rewrite_prompt"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:34:46.358636Z","iopub.status.busy":"2024-03-07T14:34:46.358351Z","iopub.status.idle":"2024-03-07T14:34:46.367556Z","shell.execute_reply":"2024-03-07T14:34:46.366644Z","shell.execute_reply.started":"2024-03-07T14:34:46.358612Z"},"trusted":true},"outputs":[],"source":["def collate_fn(batch):\n","    inputs = [torch.tensor(b['input_ids']) for b in batch]\n","    labels = [torch.tensor(b['labels']) for b in batch]\n","\n","    max_length = max(len(input_) for input_ in inputs)\n","\n","    input_ids = pad_sequence(inputs, batch_first=True, padding_value=tokenizer.pad_token_id)\n","\n","    padded_labels = []\n","    for label in labels:\n","        padded_label = torch.full((max_length,), fill_value=-100, dtype=torch.long)\n","        padded_label[:len(label)] = label\n","        padded_labels.append(padded_label)\n","    padded_labels = torch.stack(padded_labels)\n","\n","    attention_mask = torch.ones(input_ids.shape, dtype=torch.long)\n","    attention_mask[input_ids == tokenizer.pad_token_id] = 0\n","\n","    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': padded_labels}\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:34:46.368907Z","iopub.status.busy":"2024-03-07T14:34:46.368641Z","iopub.status.idle":"2024-03-07T14:34:46.378598Z","shell.execute_reply":"2024-03-07T14:34:46.377738Z","shell.execute_reply.started":"2024-03-07T14:34:46.368881Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<function collate_fn at 0x76851d8a3370>\n"]}],"source":["print(collate_fn)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:34:46.379858Z","iopub.status.busy":"2024-03-07T14:34:46.379580Z","iopub.status.idle":"2024-03-07T14:34:46.388744Z","shell.execute_reply":"2024-03-07T14:34:46.387945Z","shell.execute_reply.started":"2024-03-07T14:34:46.379832Z"},"trusted":true},"outputs":[],"source":["# import torch\n","# from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig\n","\n","\n","# # Found a good blog to catch me up fast!\n","# # https://huggingface.co/blog/4bit-transformers-bitsandbytes\n","# # https://huggingface.co/docs/transformers/v4.38.1/en/quantization#compute-data-type\n","# quantization_config = BitsAndBytesConfig(\n","#     load_in_4bit = True,\n","#     bnb_4bit_quant_type=\"nf4\",\n","#     bnb_4bit_compute_dtype=torch.bfloat16,\n","#     bnb_4bit_use_double_quant=True,\n","# )\n","\n","\n","\n","# model = AutoModelForCausalLM.from_pretrained(\n","#     MODEL_PATH,\n","#     device_map = \"auto\",\n","#     trust_remote_code = True,\n","#     quantization_config=quantization_config,\n","# )\n","\n","# # model = model.to_bettertransformer()\n","# model = accelerator.prepare(model)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:34:46.403559Z","iopub.status.busy":"2024-03-07T14:34:46.403334Z","iopub.status.idle":"2024-03-07T14:34:46.415815Z","shell.execute_reply":"2024-03-07T14:34:46.414865Z","shell.execute_reply.started":"2024-03-07T14:34:46.403539Z"},"trusted":true},"outputs":[],"source":["def del_past_models(save_path, file_exten='pth'):\n","    \"\"\"\n","    Remove all of the past models\n","    \n","    You can change the file_extension if you save the models with other file_extension\n","    \"\"\"\n","    past_models = glob.glob(os.path.join(save_path, '*.' + file_exten))\n","    for past_model in past_models:\n","        os.remove(past_model)\n","        logger.info(f'Remove model {past_model}!')\n","        \n","def save_checkpoint(path, model, optim, sched, epoch, iters):\n","    lr = optim.param_groups[0]['lr']\n","    # model_state = model.state_dict()\n","    model_state = OrderedDict((name, param) for name, param in model.named_parameters() \\\n","                    if param.requires_grad)\n","    \n","    checkpoint_path = os.path.join(path, f'checkpoint_{iters + 1}.pth')\n","    new_checkpoint_path = os.path.join(path, f'checkpoint.pth')\n","    logger.info(f\"Model of epoch {epoch} saved at checkpoint_{iters + 1}.pth, lr={lr:.3e}\")\n","    \n","    torch.save({\n","        'model': model_state,\n","        'optimizer': optim.state_dict(),\n","        'scheduler': sched.state_dict(), \n","        'epoch': epoch\n","    }, checkpoint_path)\n","    \n","    torch.save({\n","        'model': model_state,\n","        'optimizer': optim.state_dict(),\n","        'scheduler': sched.state_dict(), \n","        'epoch': epoch\n","    }, new_checkpoint_path)\n","\n","    \n","def load_checkpoint(checkpoint_path, model, optim=None, sched=None):\n","    checkpoint = torch.load(checkpoint_path)\n","    logger.info(f\"Model of epoch {checkpoint['epoch']} is loaded\")\n","    \n","    model.load_state_dict(checkpoint['model'], strict=False)\n","    if optim is not None and sched is not None:\n","        optim.load_state_dict(checkpoint['optimizer'])\n","        sched.load_state_dict(checkpoint['scheduler'])\n","        return model, optim, sched, checkpoint['epoch']\n","    else:\n","        return model, checkpoint['epoch']"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:41:09.200753Z","iopub.status.busy":"2024-03-07T14:41:09.200368Z","iopub.status.idle":"2024-03-07T14:41:09.223448Z","shell.execute_reply":"2024-03-07T14:41:09.222511Z","shell.execute_reply.started":"2024-03-07T14:41:09.200725Z"},"trusted":true},"outputs":[],"source":["def train_epoch(epoch, model, accelerator, \n","                train_dataloader, checkpointing_steps, \n","                optimizer, lr_scheduler, save_path):\n","    global overall_step\n","    model.train()\n","    epoch_loss = []\n","    pbar = tqdm(train_dataloader)\n","    grad = torch.tensor(0.0)\n","    \n","    output_dir = f\"step_{overall_step}.pth\"\n","    unwrapped_model = accelerator.unwrap_model(model)\n","    save_checkpoint(save_path, unwrapped_model, optimizer, lr_scheduler, \n","                   epoch, overall_step)\n","    \n","    for step, batch in enumerate(pbar):\n","        if (datetime.datetime.now() - start_time) > datetime.timedelta(hours=6):\n","            break\n","        with accelerator.accumulate(model):\n","            # Gradient accumulation\n","            # with accelerator.autocast():\n","            outputs = model(**batch)\n","            loss = outputs.loss\n","            accelerator.backward(loss)\n","            if accelerator.sync_gradients:\n","                grad = accelerator.clip_grad_norm_(parameters=model.parameters(), \n","                                                   max_norm=2.0)\n","            optimizer.step()\n","            lr_scheduler.step()\n","            optimizer.zero_grad()\n","\n","        overall_step += 1\n","        lr = optimizer.param_groups[0]['lr']\n","        \n","        pbar.set_description(\n","            f'Epoch {epoch}: loss = {loss.item(): .3f}, grad = {grad.item(): .3f}, lr = {lr: .3e}')\n","        \n","        with torch.no_grad():\n","            avg_loss = accelerator.gather(loss.repeat(len(batch))).mean()\n","        epoch_loss.append(avg_loss.item() / accelerator.gradient_accumulation_steps)\n","        \n","        accelerator.wait_for_everyone()\n","        if overall_step % checkpointing_steps == 0:\n","            logger.info(\n","                f'Epoch {epoch}: loss = {loss.item(): .3f}, grad = {grad.item(): .3f}, lr = {lr: .3e}')\n","            if accelerator.is_local_main_process:\n","                # Clear all of the current models\n","                del_past_models(save_path)\n","\n","                output_dir = f\"step_{overall_step}.pth\"\n","                unwrapped_model = accelerator.unwrap_model(model)\n","                save_checkpoint(save_path, unwrapped_model, optimizer, lr_scheduler, \n","                               epoch, overall_step)\n","    \n","    # Just log the loss of the main process\n","    if len(epoch_loss) > 0:\n","        logger.info(f'Epoch {epoch}: loss = {sum(epoch_loss) / (len(epoch_loss)+1e-10): .3f}, lr = {lr: .3e}')\n","    \n","            \n","def main(batch_size: int, num_epochs: int, lr: float, grad_accumulation_steps: int, \n","         checkpointing_steps: int, save_path: str, ckpt_path: str,\n","         num_warmup_steps: int=0, r: int=4, lora_alpha: int=32, lora_dropout: float=0.1):\n","    set_seed(1234)\n","    \n","    accelerator = Accelerator(gradient_accumulation_steps=grad_accumulation_steps)\n","    quantization_config = BitsAndBytesConfig(\n","        load_in_4bit=True, \n","        # bnb_4bit_quant_type=\"nf4\",\n","        bnb_4bit_compute_dtype=torch.bfloat16,\n","        # bnb_4bit_use_double_quant=True,\n","    )\n","    \n","    model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, \n","                                                 quantization_config=quantization_config, \n","                                                 torch_dtype=torch.float16)\n","    model.save_pretrained(save_path)\n","    tokenizer.save_pretrained(save_path)\n","    \n","    \n","    accelerator.print(model)\n","    \n","    # Instantiate dataloaders. (We do not split the test data)\n","    train_dataloader = DataLoader(\n","        data, shuffle=True, collate_fn=collate_fn, batch_size=batch_size\n","    )\n","    \n","    peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, \n","                             inference_mode=False, r=r, \n","                             lora_alpha=lora_alpha, lora_dropout=lora_dropout,\n","                             target_modules=\n","                            lora_target_modules_dict.get(MODEL_PATH, [\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],)\n","                            )\n","    model = get_peft_model(model, peft_config)\n","    \n","    if accelerator.is_local_main_process:\n","        model.print_trainable_parameters()\n","    \n","    lr = lr * accelerator.num_processes * accelerator.gradient_accumulation_steps\n","    \n","    optimizer = AdamW(params=model.parameters(), lr=lr)\n","    \n","    total_steps = len(train_dataloader) * num_epochs * \\\n","            accelerator.num_processes * accelerator.gradient_accumulation_steps\n","    \n","    # Instantiate scheduler\n","    lr_scheduler = get_cosine_schedule_with_warmup(\n","        optimizer=optimizer,\n","        num_warmup_steps=num_warmup_steps,\n","        # to ensure the lr will not become zero at the end of training\n","        # (You can adjust this param)\n","        num_training_steps=total_steps * 1.1,\n","    )\n","    \n","    current_epochs = 0\n","    global overall_step\n","    overall_step = 0\n","    \n","    # Load checkpoint\n","    print(ckpt_path)\n","    if os.path.isfile(ckpt_path):\n","        print('loading')\n","        model, optimizer, lr_scheduler, current_epochs = \\\n","                    load_checkpoint(ckpt_path, model, optimizer, lr_scheduler)\n","        c = re.search('(\\d)+', ckpt_path)\n","        if c is None: overall_step = 1\n","        else: overall_step = int(c.group())\n","        logger.info(\n","            f'Checkpoint {ckpt_path} loaded at epoch {current_epochs}, the training will resume from epoch {current_epochs + 1}!')\n","        current_epochs += 1\n","    \n","    \n","    if current_epochs >= num_epochs:\n","        raise ValueError('The num_epochs should be larger than the saved epochs!!')\n","    \n","    # Prepare everything\n","    # There is no specific order to remember, \n","    # we just need to unpack the objects in the same order we gave them to the prepare method.\n","    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n","        model, optimizer, train_dataloader, lr_scheduler\n","    )\n","    \n","    \n","    logger.info('*********************** Start training! **************************')\n","    \n","    for epoch in range(current_epochs, num_epochs):\n","        train_epoch(epoch, model, accelerator, train_dataloader, \n","                    checkpointing_steps, optimizer, lr_scheduler, save_path)\n","        \n","    # Save the final model\n","    accelerator.wait_for_everyone()\n","    if accelerator.is_local_main_process:\n","        output_dir = f\"step_{overall_step}.pth\"\n","        unwrapped_model = accelerator.unwrap_model(model)\n","        save_checkpoint(save_path, unwrapped_model, optimizer, lr_scheduler, \n","                       epoch, overall_step)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["lora_target_modules_dict = {\n","  'gpt2': ['c_attn'],\n","  'distilbert/distilgpt2': ['c_attn'],\n","  'distilbert/distilroberta-base': ['query', 'key', 'value'],\n","}\n","import json\n","os.makedirs('./settings', exist_ok=True)\n","json.dump(lora_target_modules_dict, open('./settings/lora_target_modules.json', 'w'))"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:34:46.442375Z","iopub.status.busy":"2024-03-07T14:34:46.442144Z","iopub.status.idle":"2024-03-07T14:34:46.455275Z","shell.execute_reply":"2024-03-07T14:34:46.454417Z","shell.execute_reply.started":"2024-03-07T14:34:46.442355Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["50256\n"]}],"source":["if tokenizer.pad_token_id is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","print(tokenizer.pad_token_id)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["import logging\n","import os\n","import datetime\n","os.makedirs('./logs/', exist_ok=True)\n","logging.basicConfig(\n","  level=logging.INFO,\n","  filename='./logs/log_%s.txt' % datetime.datetime.now().strftime('%y%m%d%H%m%S'), filemode='a',\n","  datefmt='%H:%M:%S',\n","  format='%(asctime)s - %(levelname)s - %(message)s',\n",")\n","\n","logger = logging.getLogger(__name__)\n","# Stream handler, logging to the stream\n","logger.info(\"abc\")"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Jupyter Capture Output v0.0.11\n"]}],"source":["import jupyter_capture_output"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:41:10.458119Z","iopub.status.busy":"2024-03-07T14:41:10.457747Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Output saved by overwring previous file at cap/overfitting.txt.\n"]},{"name":"stderr","output_type":"stream","text":["`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7791857c670d48d1a343b44a917b4df5","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e4c5b0f10dcc434e878d99149f19a33a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"19c8ba1d34e4494990d4c1d7a6c4bd5f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ff04cb7952634c88ad844c231ea30e6c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4bd1e159723b46bbb77712d4ae92f5ee","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cdf37380ce314119982802d927a3f12c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6de75267878e49d18240ce2b7304a9b8","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"83dcce89fe894591b5186f7d6ecfef9c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"598acb3b06cb47a4a5b578cc7282dd15","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dc9eb38fee904eee8f7ee4af5d30ab6d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d4a010bb224542eeb73e80589964faf9","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d13748a66a2f4f1b9fd80966ad6a9b82","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4ff70d60ba3245a9a60b894a82a08fe6","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6ffdf84a65e841889b32e4341366c720","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8cb0c8f9417d4212bc0bd9cb079d6322","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6df5f31cd6324a7ea7c49e9952766e57","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"330d5b8dac054fd3a9a203b177674ba1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1891e5db4cab4d0b9d4dfb6f1cc1b290","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6e6fbfbd169042acb2a1472a812d638c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0719c0a73d32444eace6d700ce2e9bdc","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3f470194c2894b43b02120030bb9ff16","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2fc6470f68524e64bb7b8f1bcafc812f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fbeb8631391c4fe7a4d23831a1e9bea8","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"52ec06810b414f18906ef35c4451deeb","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"665519cc02ae4e878ea5e0575e5b6b07","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c39eff722637407b80eac629d84686fd","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e436d8cae39e4678a86aa228d0cc3cfd","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"26e70c89fa684dc697a513dd6206d7ed","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d1ae509b92e4402487af663c95e6ef8d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"50a649df341e44e89f00e7ef765d6049","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ab97db8fa13b40549d5df90c2db6f393","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9cda96c7c6d64114983e11dbd808741e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e6fd27bdb52541eb996dc4eef0ee10eb","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f4bea94c45fb4123935573e89c8b96fd","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cd0631e419fc45a3b341243bde3b9e70","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"548004d7d12640b3ab3c39ccf2e48dd5","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0b94ef512af44986b0957b279b0b067f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"077594ca1e054ddb86a0cb7b6333edf5","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"725d48a0600a493883526f0b261abe54","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c81fb0ecca364cfa829d77aafa31b061","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f9ed3d8918254783ae3eb7957d89ffe3","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"de5374b30ebd4c3893088c32d0656a37","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"01586a1eee564a738b11bceaa4f4a01c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4737ae56f3fc4eb387905a9d367eeba3","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6d8a4c14ea1a47749d009f89c34571dc","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"535f7ba18bd045eaa5f4354ba1ced94e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a95ad0ce0785443db49286c43b7de622","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8c6f030c87b2486aaf63980352e73fa7","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d049182249854cef99966005f0a76fb5","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"968d63a10df644e0bd585e8413391be9","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"89a02ae47717457980f5cd75b76b7e3a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"12c4ffa8bf7a4dfab5b60ea80ae214d7","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"04bbe377a9c14a32a7a62f5663642196","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1b29d01a497c4780ae7c0ea4f46e9ae6","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d90b9ee30ec446bba7af77f618e30cb8","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bc365f9c896d4f7585cc793f0ac36b38","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e9e94aba427c4de9a4b16d52ac21ddbc","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ce2ece6dd2ba42bf906edcc5e1d98829","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6f379a7812e446cea45ddc161c60e573","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"93d5f0ff0ba04ac8a255cf5ccac20455","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"172e319ac5954542b58723fda3a67b35","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f00c63a8a5e94df0ab929cca0c1515f5","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c4400c88b57c4d08ad6faa6ffb2ba99b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a16b606107684fcfa648b1384da587fd","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"38ffe73ceb1d4b2691dae00211b8ee54","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"54110de7693548f1918e69a6ceb9b1b3","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ae4184909a5c4da095f40050201b0edd","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c7e14438eac944ac89feee7dd29366de","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"018b0b5903bf4952b548b8370f5bc797","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ad51c949b3b041839b1e200ba62044d1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"985be299124b495495635e591838f553","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ef0650b60ff84ac9b0acdcf2da4f6e23","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bc41f27d5a49445fa6473b89d495cae3","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"565637ca1b754c90a213f56b0ab7b020","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c5d12c153023404e80192b4a74b9a326","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"906af6821dd1497ab53b9a518e15d374","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"20590e80703a4ce491d9aacd866aa633","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"50031a23301b42998e57c2c7aa077be1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3bbdd4962d014fc3a2cb7d42a83ea6f7","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d95c8b60c47d4ea6b99c56a36082305a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ad973fe64fb145889b40c54ca0ba0176","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"46e3bf457aa945c5a8693fa7ad5d800a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"088439739889450bb4e9759d66b30e86","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"849d6c80801e436faafcce684156dc61","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f71e4695132d4c5aacc703a376347feb","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b1b8c6b78b57473492cfad9ff1fee103","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a33aea1befd6457986a64a9ccdb631a3","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8d574e0de6a74e6a85d00880c905c023","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"208672a7d68b49d4a8f1189e7a3498a4","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5defc5fe4488431ea789782317549790","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"591288fb141f44c7ac481a3bcdb796d5","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"727b42227f9e43369457c2d3f00bb626","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6e050a539559463eb63a69eb26828f2a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bb95c916c6a3435da3f5b59dd3f0d490","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cd8c2d14d88d457d868d7054daf66421","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4e8d4dad8bdf484787f5385d17d25e0d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d0466f94a7ef4b36bb7dd00c29404445","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bb85410e20d64264ac64245e7bb29633","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2d263ef9f7df4a089b74e3f5b5fba99c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0ace23f9cf5a43fb8dadac18d869464f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a109cae212fa4b4394f25e6b43eb18d6","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"205047d8c00d4c33bc078bf6650e7533","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c8fe9251d9274ca3b3acac371ec6354e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a053b43e504b40d0906404f5ecf9b63d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c6f07a7b618946c0856777ee625ccd58","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f39fdafe354d4621914aab5c0ecb5cd0","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1fc69e69fca0487098f601b9023f0d31","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"84e38fb183b3452e85fb0f4e83c5b8ff","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"78af43a70f994c20b6fbb1ce5280b9bb","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dd3dcc2cc11d4de1846d60b1cf30cc72","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"27960992b1cc44adb7a356ac94d55b58","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5bf5636e649a4872ad1bca119c04956e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"63bd12b55e3442ae8f0ff673015e05a1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"64e6c2cda4c14aadb8382debf2d504c6","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5ee1b83124344b4a8b744f3b916b846e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"46753c45503245ee97076c9c5bc58dca","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f82c4a5308f1464e8f78ccc75ebb7553","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1200dc4a862a45f39af35046ab7c8bd8","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f929b9e31b254560a12c1fa54772ae52","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"361663c43d3e443db7dbdd7f671bbdfe","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9ef690171df94c5682a9a8d850d4cd5b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e733127819c64966bd3fea3aa94adf7d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c73f4023ef1c49eab9c060e8a193a7f3","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"044f5741f3984652931e899cc0b41ff7","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b92270bceaf6486ca0edb1ac96e8b7b3","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d83c3d82ff844eb4915297494ea50428","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f51f9145d0ea4e3fb3ca0a4042f6beb8","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f76788cbbd13406ba859ca3ba0ff7425","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"575d9151ddef46b2b2abc4bc5ff1914f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"951f95ec392246779ea7367abefe720b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9d433ac8878c46a18c17922dbf8b5ba3","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1fe4f41f1b664223a6ce6f97ae2cfd6d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4b50627f9d354f7cb725d45cd521dbb2","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a91b8e85aae9427e81e73709250b0e66","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ae39a2290724434eaefe81a2ed1deeb4","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"83349f9959c04ebf9335dd94c4f246f5","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"42fc6d6d497e45deb46fa761eef9675d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"85fe3134276b4bd083755c8fe66f1095","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d96fe420402c440b8361c67c4b3e91c8","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9f664ab6bb6b4909bd7d3c8bb590d0b5","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"34cb67dc5537432d907db4805c581a75","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8c74e3086dd346a99abd9e4afe7ca999","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"72dcc767603741adad4272cb5f6b75cc","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3f8930a57c1f4e84a3abe28c16d93576","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"535edfe9895f48aea913386e498e3795","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f8b10599c5b04f0d870f58e3633a981c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3fb9f63bfb67430bb5494260f439424f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e8318a047ad048068cfa5ba5a1dad0b5","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c321da289af443958617aa396aaec5c0","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d9db63e69bf04aa6a42d49fedde4c357","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5939fbea0e4546c0aaaaf907c14a03cc","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a4b98516206c4cfda01ce1b03d2aa200","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c9a348da07744969b72e209b27552b6a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c59c0ce351eb40f4b4c4394acaf85cc6","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8abcc06c795d412fa4ac8351c0882693","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0bd9921531494accb4217dd1d412f260","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["%%capture_text --path \"cap/overfitting.txt\"\n","\n","import os\n","\n","batch_size = 2\n","grad_accumulation_steps = 1\n","num_epochs = 600\n","lr = 5e-5\n","checkpointing_steps = 500\n","save_path = os.path.join('./working/trained_models/', MODEL_PATH)\n","r = 64\n","lora_alpha = 64\n","lora_dropout = 0.05\n","\n","# If ckpt_path is a real path (os.path.isfile(ckpt_path) is True),\n","# then the checkpoint will be loaded\n","ckpt_path = os.path.join(save_path, 'checkpoint.pth')\n","print(ckpt_path)\n","kwargs = {\n","'batch_size':batch_size, 'num_epochs':num_epochs, 'lr':lr, 'grad_accumulation_steps':grad_accumulation_steps, \n","'checkpointing_steps':checkpointing_steps, 'save_path':save_path, 'ckpt_path':ckpt_path, 'r':r, 'lora_alpha':lora_alpha, 'lora_dropout':lora_dropout\n","}\n","\n","if not os.path.exists(save_path):\n","    os.makedirs(save_path, exist_ok=True)\n","# notebook_launcher(main, args, num_processes=1)\n","main(**kwargs)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def truncate_txt(text, length):\n","    text_list = text.split()\n","    \n","    if len(text_list) <= length:\n","        return text\n","    \n","    return \" \".join(text_list[:length])\n","\n","\n","def gen_style_prompt(og_text, rewritten_text):\n","    \n","    # Truncate the texts to first 200 words for now\n","    # As we are having memory issues on Mixtral8x7b\n","    og_text = truncate_txt(og_text, 200)\n","    rewritten_text = truncate_txt(rewritten_text, 200)\n","    \n","    return f\"\"\"    \n","    Original Essay:\n","    \\\"\"\"{og_text}\\\"\"\"\n","    \n","    Rewritten Essay:\n","    \\\"\"\"{rewritten_text}\\\"\"\"\n","    \n","    You are given 2 essays, the Rewritten essay was created from the Original essay using the google Gemma model.\n","    Analyzing the changes in style, theme, etc., please output the phrase following:\n","    \\\"Please improve the following text using the writing style of [insert author/style name]\\\"\n","    Output should be one sentence only.\n","    \"\"\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","from tqdm import tqdm\n","\n","DEBUG = True\n","\n","TEST_DF_FILE = './input/llm-prompt-recovery/test.csv'\n","SUB_DF_FILE = './input/llm-prompt-recovery/sample_submission.csv'\n","NROWS = None if DEBUG else None\n","\n","if DEBUG:\n","    TEST_DF_FILE = './input/gemma-rewrite-nbroad/nbroad-v1.csv'\n","    SUB_DF_FILE = TEST_DF_FILE\n","\n","tdf = pd.read_csv(TEST_DF_FILE, nrows=NROWS, usecols=['id', 'original_text', 'rewritten_text'])\n","sub = pd.read_csv(SUB_DF_FILE, nrows=NROWS, usecols=['id', 'rewrite_prompt'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1666 549\n"]}],"source":["maxlen, max_i = 0,0\n","for i in range(len(tdf)):\n","  clen = len(gen_style_prompt(tdf.iloc[i]['original_text'], tdf.iloc[i]['rewritten_text']).split(' '))\n","  if clen > maxlen:\n","    max_i = i\n","    maxlen = clen\n","    \n","print(max_i, maxlen)\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["495\n"]}],"source":["print(len(gen_style_prompt(tdf.iloc[772]['original_text'], tdf.iloc[772]['rewritten_text']).split(' ')))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["('./working/trained_models/gpt2/tokenizer_config.json',\n"," './working/trained_models/gpt2/special_tokens_map.json',\n"," './working/trained_models/gpt2/vocab.json',\n"," './working/trained_models/gpt2/merges.txt',\n"," './working/trained_models/gpt2/added_tokens.json',\n"," './working/trained_models/gpt2/tokenizer.json')"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.save_pretrained(save_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:39:45.155903Z","iopub.status.busy":"2024-03-07T14:39:45.155551Z","iopub.status.idle":"2024-03-07T14:39:45.160547Z","shell.execute_reply":"2024-03-07T14:39:45.159517Z","shell.execute_reply.started":"2024-03-07T14:39:45.155876Z"},"trusted":true},"outputs":[],"source":["try:\n","    del model, optimizer, train_dataloader, lr_scheduler\n","except Exception:\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:40:45.855872Z","iopub.status.busy":"2024-03-07T14:40:45.855499Z","iopub.status.idle":"2024-03-07T14:40:46.067760Z","shell.execute_reply":"2024-03-07T14:40:46.066318Z","shell.execute_reply.started":"2024-03-07T14:40:45.855842Z"},"trusted":true},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-07T14:37:47.408644Z","iopub.status.idle":"2024-03-07T14:37:47.408985Z","shell.execute_reply":"2024-03-07T14:37:47.408836Z","shell.execute_reply.started":"2024-03-07T14:37:47.408821Z"},"trusted":true},"outputs":[],"source":["import time\n","time.sleep(1000)"]},{"cell_type":"markdown","metadata":{},"source":["# Sub"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-07T14:37:47.410563Z","iopub.status.idle":"2024-03-07T14:37:47.410894Z","shell.execute_reply":"2024-03-07T14:37:47.410749Z","shell.execute_reply.started":"2024-03-07T14:37:47.410735Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","from tqdm import tqdm\n","\n","DEBUG = True\n","\n","TEST_DF_FILE = './input/llm-prompt-recovery/test.csv'\n","SUB_DF_FILE = './input/llm-prompt-recovery/sample_submission.csv'\n","NROWS = None if DEBUG else None\n","\n","if DEBUG:\n","    TEST_DF_FILE = './input/gemma-rewrite-nbroad/nbroad-v1.csv'\n","    SUB_DF_FILE = TEST_DF_FILE\n","\n","tdf = pd.read_csv(TEST_DF_FILE, nrows=NROWS, usecols=['id', 'original_text', 'rewritten_text'])\n","sub = pd.read_csv(SUB_DF_FILE, nrows=NROWS, usecols=['id', 'rewrite_prompt'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tdf = tdf.iloc[:2]\n","sub = sub.iloc[:2]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>original_text</th>\n","      <th>rewritten_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>LNpAovroGe</td>\n","      <td>This quilt, that my mother made, \\n \\n Still m...</td>\n","      <td>The softest brown and brightest blue quilt, cr...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>nnuxwwThWi</td>\n","      <td>It's the job of our agency to keep track of th...</td>\n","      <td>The agency's responsibility is to track and co...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           id                                      original_text  \\\n","0  LNpAovroGe  This quilt, that my mother made, \\n \\n Still m...   \n","1  nnuxwwThWi  It's the job of our agency to keep track of th...   \n","\n","                                      rewritten_text  \n","0  The softest brown and brightest blue quilt, cr...  \n","1  The agency's responsibility is to track and co...  "]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["tdf"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>rewrite_prompt</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>LNpAovroGe</td>\n","      <td>Regency Romance: Model the text on a Regency r...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>nnuxwwThWi</td>\n","      <td>Write like Ernest Hemingway: Focus on Hemingwa...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           id                                     rewrite_prompt\n","0  LNpAovroGe  Regency Romance: Model the text on a Regency r...\n","1  nnuxwwThWi  Write like Ernest Hemingway: Focus on Hemingwa..."]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["sub"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-07T14:37:47.412789Z","iopub.status.idle":"2024-03-07T14:37:47.413261Z","shell.execute_reply":"2024-03-07T14:37:47.413020Z","shell.execute_reply.started":"2024-03-07T14:37:47.413002Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/2 [00:00<?, ?it/s]`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","\n","No chat template is defined for this tokenizer - using the default template for the GPT2TokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n","\n","A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"]},{"name":"stdout","output_type":"stream","text":["Model of epoch 2 is loaded\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|     | 1/2 [00:02<00:02,  2.59s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","100%|| 2/2 [00:03<00:00,  1.69s/it]\n"]}],"source":["import gc\n","import re\n","\n","device = 'cuda'\n","tdf['id'] = sub['id'].copy()\n","\n","pbar = tqdm(total=tdf.shape[0])\n","\n","it = iter(tdf.iterrows())\n","idx, row = next(it, (None, None))\n","\n","# https://www.kaggle.com/competitions/llm-prompt-recovery/discussion/481116\n","DEFAULT_TEXT = 'Please improve the following text, maintaining the original meaning but altering the tone, diction, and stylistic elements to match the new style.Enhance the clarity, elegance, and impact of the following text by adopting the writing style of , ensuring the core message remains intact while transforming the tone, word choice, and stylistic features to align with the specified style.' \n","\n","res = []\n","model = None\n","while idx is not None:\n","    \n","    if (datetime.datetime.now() - start_time) > datetime.timedelta(hours=8, minutes=30):\n","        res.append([row[\"id\"], DEFAULT_TEXT])\n","        idx, row = next(it, (None, None))\n","        pbar.update(1)\n","        continue\n","        \n","    torch.cuda.empty_cache()\n","    gc.collect()\n","        \n","    try:\n","        if model is None:\n","            ckpt_path = './working/trained_models/%s/checkpoint.pth' % MODEL_PATH\n","            quantization_config = BitsAndBytesConfig(\n","                load_in_4bit=True, \n","                bnb_4bit_quant_type=\"nf4\",\n","                bnb_4bit_compute_dtype=torch.bfloat16,\n","                bnb_4bit_use_double_quant=True,\n","            )\n","            model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, \n","                                                         quantization_config=quantization_config, \n","                                                         torch_dtype=torch.float16)\n","            checkpoint = torch.load(ckpt_path)\n","\n","            model.load_state_dict(checkpoint['model'], strict=False)\n","            print(f\"Model of epoch {checkpoint['epoch']} is loaded\")\n","        \n","        \n","        model.eval()    \n","            \n","            \n","        messages = [\n","            {\n","                \"role\": \"user\",\n","                \"content\": gen_prompt(row[\"original_text\"], row[\"rewritten_text\"])\n","            }\n","        ]\n","        encoded_input = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(device)\n","        \n","        with torch.no_grad():\n","            encoded_output = model.generate(encoded_input, max_new_tokens=50, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n","        \n","        decoded_output = tokenizer.batch_decode(encoded_output, skip_special_tokens=True)[0]\n","        decoded_output = result = re.sub(r\"[\\s\\S]*\\[\\/INST\\]\", '', decoded_output, 1)\n","                \n","        res.append([row[\"id\"], decoded_output])\n","                            \n","    except Exception as e:\n","        print(f\"ERROR: {e}\")\n","        res.append([row[\"id\"], DEFAULT_TEXT])\n","        \n","    finally:\n","        idx, row = next(it, (None, None))\n","        pbar.update(1)\n","\n","        \n","pbar.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-07T14:37:47.414902Z","iopub.status.idle":"2024-03-07T14:37:47.415620Z","shell.execute_reply":"2024-03-07T14:37:47.415397Z","shell.execute_reply.started":"2024-03-07T14:37:47.415377Z"},"trusted":true},"outputs":[],"source":["# sub[\"rewrite_prompt\"] = tdf['rewrite_prompt'].copy()\n","# sub.to_csv(\"submission.csv\", index=False)\n","sub = pd.DataFrame(res, columns=['id', 'rewrite_prompt'])\n","\n","sub.to_csv(\"sample_submission.csv\", index=False)\n","sub.to_csv(\"submission.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-07T14:37:47.416641Z","iopub.status.idle":"2024-03-07T14:37:47.417090Z","shell.execute_reply":"2024-03-07T14:37:47.416867Z","shell.execute_reply.started":"2024-03-07T14:37:47.416848Z"},"trusted":true},"outputs":[],"source":["sub"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-07T14:37:47.419040Z","iopub.status.idle":"2024-03-07T14:37:47.419499Z","shell.execute_reply":"2024-03-07T14:37:47.419288Z","shell.execute_reply.started":"2024-03-07T14:37:47.419269Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[['LNpAovroGe',\n","  'Original Text:\\n    \"\"\"This quilt, that my mother made, \\n \\n Still makes me think to this day. \\n \\n It\\'s softest brown, and brightest blue, \\n \\n The curved stitch here, reads `` made it May\\'\\'. \\n \\n It\\'s hard to see, but believe me it\\'s true, \\n \\n That\\'s not just a cloth but a piece of shirt. \\n \\n You can see a logo here, and right there, \\n \\n And a signature over there, someone named `` Bert\\'\\'. \\n \\n This is my favorite part, a piece from a stuffed bear. \\n \\n I think it was my mother\\'s favorite too, \\n \\n She always said so at least. \\n \\n Something from when she was two, \\n \\n Given by her grandad for Thanksgiving feast. \\n \\n My dad added this, a little button pin, \\n \\n Something from his mother, for being a scout. \\n \\n Apparently she went to a store and fished in a bin, \\n \\n Until night that day, to teach him what love was about. \\n \\n I\\'m sorry you had to see this, \\n \\n but their funeral was delayed. \\n \\n \\n \\n\\n\"\"\"\\n    \\n    Rewritten Text:\\n    \"\"\"The softest brown and brightest blue quilt, crafted by the loving hand of a mother, evokes vivid memories in my mind. The curved stitch, delicately etched upon its surface, bears the inscription \"made it May,\" a testament to the passage of time. Though the fabric may be veiled in mystery, I firmly believe that this quilt is not merely a cloth but a treasured heirloom, imbued with love and sentiment. The intricate logo and signature, meticulously woven into the quilt\\'s fabric, reveal the name \"Bert,\" a symbol of the owner\\'s identity. This precious artifact, once a part of a stuffed bear, holds a special place in my heart. It was my mother\\'s favorite, a cherished relic from her youth, gifted to her by her grandfather during a Thanksgiving feast. My father added his own touch to the quilt, a pinned button that symbolized his mother\\'s unwavering spirit and her dedication to scouting. Apparently, she spent the entire night fishing in a store bin, determined to teach her son about the\"\"\"\\n    \\n    You are given 2 essays, the Rewritten text was created from the Original text using the google Gemma model.\\n    Analyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten text.\\n    Start directly with the prompt, output should be one line only.A young woman went on a five day journey when she met her father on a night out to see what was going on with her life before the death. The person who met with her father to get her out of hiding and try to get her out'],\n"," ['nnuxwwThWi',\n","  'Original Text:\\n    \"\"\"It\\'s the job of our agency to keep track of the worlds monsters and keep them locked down. Which is complicated because people aren\\'t allowed to know they exist or that they create them. Fear itself, when amplified by a large amount of people, fearing that same thing enough... produces a monster to represent it, it simply comes to life and exists. We stop said Monsters from destroying us. We always do. This time just over a decade ago, our job got a lot harder, a new breed of monster had appeared, and it was more dangerous than we ever suspected possible. The thing about the old monsters is they\\'re like the fear they represent: Werewolves were simply animals, animals that stick to the forest, they\\'re easy to explain to locals and generally don\\'t even cause much trouble so long as they\\'re left well enough alone, we picked off the aggressive ones and left the rest alone. Vampires were a problem for a long while, because like the\"\"\"\\n    \\n    Rewritten Text:\\n    \"\"\"The agency\\'s responsibility is to track and contain the monstrous creatures that roam the world. This task proves challenging due to the taboo surrounding their existence and the fear they induce. Fear, amplified by a large number of individuals, breeds a monstrous representation of that fear. We engage in combat against these creatures, ensuring their destruction. Recently, a decade ago, our work became infinitely more difficult. A new breed of monster emerged, one more menacing than any we had encountered previously. Unlike the old monsters, these creatures are like the fear they represent: animals that embody the wildness of nature, vampires that exploit their power to prey upon others, and zombies whose existence is rooted in the fear of disease. However, our latest adversary presents a unique challenge. Unlike the old monsters, which are relatively straightforward to identify and defeat, these creatures self-destruct mere seconds after their creation. Their existence is fleeting, yet their destructive force is undeniable. It is this new monster that has us grappling with the complexities\"\"\"\\n    \\n    You are given 2 essays, the Rewritten text was created from the Original text using the google Gemma model.\\n    Analyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten text.\\n    Start directly with the prompt, output should be one line only.Cities and suburbs all hate their own cities and are in the grips of a rising tide of \"populist madness that threatens to eclipse and even destroy all that\\'s left.\"\\n\\nThe movement, called \"populism to save cities and']]"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["res"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":7806901,"sourceId":67121,"sourceType":"competition"},{"datasetId":4506214,"sourceId":7747717,"sourceType":"datasetVersion"},{"sourceId":140952720,"sourceType":"kernelVersion"},{"sourceId":164836055,"sourceType":"kernelVersion"},{"modelInstanceId":284,"sourceId":386,"sourceType":"modelInstanceVersion"},{"modelInstanceId":3900,"sourceId":5112,"sourceType":"modelInstanceVersion"},{"modelInstanceId":4761,"sourceId":5994,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelInstanceId":8332,"sourceId":11394,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
