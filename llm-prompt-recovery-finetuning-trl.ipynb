{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-03-07T14:34:34.653535Z","iopub.status.busy":"2024-03-07T14:34:34.652850Z","iopub.status.idle":"2024-03-07T14:34:40.781469Z","shell.execute_reply":"2024-03-07T14:34:40.780703Z","shell.execute_reply.started":"2024-03-07T14:34:34.653491Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/automl/duckle/eq_gen/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["from accelerate.utils import BnbQuantizationConfig\n","from accelerate import Accelerator, notebook_launcher\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, \\\n","                        get_cosine_schedule_with_warmup, set_seed\n","import transformers\n","import optimum\n","\n","from datasets import load_dataset,Dataset\n","import pandas as pd\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, \\\n","                        get_cosine_schedule_with_warmup, set_seed\n","from peft import LoraConfig, TaskType, get_peft_model\n","from accelerate import Accelerator, notebook_launcher\n","from accelerate.utils import set_seed\n","import logging\n","from torch.utils.data import DataLoader\n","from torch.optim import AdamW, SGD\n","from tqdm.notebook import tqdm\n","import torch\n","from torch.nn.utils.rnn import pad_sequence\n","import glob\n","from collections import OrderedDict\n","import re\n","\n","import os\n","\n","import pandas as pd\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:34:40.783110Z","iopub.status.busy":"2024-03-07T14:34:40.782634Z","iopub.status.idle":"2024-03-07T14:34:40.787187Z","shell.execute_reply":"2024-03-07T14:34:40.786265Z","shell.execute_reply.started":"2024-03-07T14:34:40.783080Z"},"trusted":true},"outputs":[],"source":["import datetime\n","start_time = datetime.datetime.now()"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def gen_train_test():\n","  df = pd.concat([\n","    # pd.read_csv(\"./input/gemma-rewrite-nbroad/nbroad-v1.csv\"),\n","    # pd.read_csv(\"./input/gemma-rewrite-nbroad/nbroad-v2.csv\"),\n","    pd.read_csv(\"./input/mydata/train_part1.csv\"),\n","  ]).reset_index(drop=True)\n","\n","  from sklearn.model_selection import train_test_split\n","\n","  train, test = train_test_split(df, test_size=0.1)\n","  # train.to_csv('./input/gemma-rewrite-nbroad/train.csv')\n","  # test.to_csv('./input/gemma-rewrite-nbroad/test.csv')\n","  train.to_csv('./input/mydata/train.csv')\n","  test.to_csv('./input/mydata/test.csv')"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:34:42.360542Z","iopub.status.busy":"2024-03-07T14:34:42.360264Z","iopub.status.idle":"2024-03-07T14:34:46.357229Z","shell.execute_reply":"2024-03-07T14:34:46.356439Z","shell.execute_reply.started":"2024-03-07T14:34:42.360518Z"},"trusted":true},"outputs":[],"source":["# MODEL_PATH = \"/kaggle/input/gemma/transformers/7b-it/2\"\n","# MODEL_PATH = \"distilbert/distilgpt2\"\n","# MODEL_PATH = \"gpt2\"\n","# MODEL_PATH = \"microsoft/deberta-v3-base\"\n","# MODEL_PATH = \"distilbert/distilroberta-base\"\n","MODEL_PATH = 'mistralai/Mistral-7B-Instruct-v0.2'\n","# MODEL_PATH = \"google/gemma-2b-it\"\n","# MODEL_PATH = \"google/flan-t5-small\"\n","# MODEL_PATH = \"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\"\n","# MODEL_PATH = \"/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1\"\n","# MODEL_PATH = \"/kaggle/input/llama-2/pytorch/7b-chat-hf/1\"\n","# MODEL_PATH = \"/kaggle/input/llama-2/pytorch/13b-chat-hf/1\"\n","\n","max_length = 1024\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n","tokenizer.model_max_length = max(tokenizer.model_max_length, max_length)\n","\n","    "]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 4109/4109 [00:04<00:00, 927.53 examples/s] \n","Map: 100%|██████████| 4109/4109 [00:02<00:00, 1379.21 examples/s]\n","Map: 100%|██████████| 4109/4109 [00:00<00:00, 15164.30 examples/s]\n","Map: 100%|██████████| 457/457 [00:00<00:00, 1187.83 examples/s]\n","Map: 100%|██████████| 457/457 [00:00<00:00, 2059.82 examples/s]\n","Map: 100%|██████████| 457/457 [00:00<00:00, 15362.52 examples/s]"]},{"name":"stdout","output_type":"stream","text":["4109 457\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["def create_data(df, tokenizer, split='train'):\n","  data = Dataset.from_pandas(df[[\n","    # 'reverse_prompt',\n","    'rewrite_prompt', 'original_text', 'rewritten_text'\n","  ]],split=split)\n","\n","\n","  # def tokenize_samples(samples):\n","  #   inputs = tokenizer(samples[\"reverse_prompt\"], max_length=max_length, truncation=True)\n","  #   targets = tokenizer(samples[\"rewrite_prompt\"], max_length=max_length, truncation=True)\n","  #   return {'input_ids': inputs['input_ids'], 'attention_mask': inputs['attention_mask'], 'labels': targets['input_ids']}\n","  # data = data.map(tokenize_samples, batched=True)\n","  data = data.map(lambda samples: tokenizer(samples[\"original_text\"], max_length=max_length, truncation=True), batched=True)\n","  data = data.map(lambda samples: tokenizer(samples[\"rewritten_text\"], max_length=max_length, truncation=True), batched=True)\n","  data = data.map(lambda samples: tokenizer(samples[\"rewrite_prompt\"], max_length=max_length, truncation=True), batched=True)\n","  return data\n","\n","train, test = (\n","  create_data(pd.read_csv('./input/gemma-rewrite-nbroad/train.csv'), tokenizer, 'train'),\n","  create_data(pd.read_csv('./input/gemma-rewrite-nbroad/test.csv'), tokenizer, 'test'),\n",")\n","print(len(train), len(test))"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["\n","def truncate_txt(text, length):\n","    text_list = text.split()\n","    \n","    if len(text_list) <= length:\n","        return text\n","    \n","    return \" \".join(text_list[:length])\n","\n","\n","def gen_prompt(og_text, rewritten_text, truncate_length=200):\n","    \n","    # Truncate the texts to first 200 words for now\n","    # As we are having memory issues on Mixtral8x7b\n","    og_text = truncate_txt(og_text, truncate_length)\n","    rewritten_text = truncate_txt(rewritten_text, truncate_length)\n","    \n","    return f\"\"\"\n","You are given 2 essays, the Rewritten essay was created from the Original essay using the google Gemma model.\n","Analyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten essay.\n","Start directly with the prompt, output should be one line only.\n","\n","Original Essay:\n","\\\"\"\"{og_text}\\\"\"\"\n","\n","Rewritten Essay:\n","\\\"\"\"{rewritten_text}\\\"\"\"\n","\n","\"\"\".strip()\n","\n","def formatting_func(example):\n","  output_texts = []\n","  for i in range(len(example['original_text'])):\n","    prompt = tokenizer.apply_chat_template(\n","      [{\n","        'role': 'user',\n","        'content' : gen_prompt(example['original_text'][i], example['rewritten_text'][i]) + '\\n ',\n","      }],\n","      tokenize=False,\n","    )\n","    text = f\"{prompt}{example['rewrite_prompt'][i]}{tokenizer.eos_token}\"\n","    output_texts.append(text)\n","  return output_texts"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[2024-03-12 21:38:56,320] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","<s>[INST] [/INST]\n"]}],"source":["from trl import DataCollatorForCompletionOnlyLM\n","instruction_template, response_template = tokenizer.apply_chat_template(\n","  [{\n","    'role':'user',\n","    'content': 'abc123'\n","  }],\n","  tokenize=False,\n",").split('abc123')\n","instruction_template, response_template = instruction_template.strip(), response_template.strip()\n","print(instruction_template, response_template)\n","collator = DataCollatorForCompletionOnlyLM(response_template=response_template,\n","                                           tokenizer=tokenizer,)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:41:09.200753Z","iopub.status.busy":"2024-03-07T14:41:09.200368Z","iopub.status.idle":"2024-03-07T14:41:09.223448Z","shell.execute_reply":"2024-03-07T14:41:09.222511Z","shell.execute_reply.started":"2024-03-07T14:41:09.200725Z"},"trusted":true},"outputs":[],"source":["from trl import SFTTrainer\n","\n","def main(batch_size: int, num_epochs: int, lr: float, grad_accumulation_steps: int, \n","         checkpointing_steps: int, save_path: str, ckpt_path: str,\n","         num_warmup_steps: int=0, r: int=4, lora_alpha: int=32, lora_dropout: float=0.1,\n","         eval_steps=None):\n","    set_seed(1234)\n","    \n","    accelerator = Accelerator(gradient_accumulation_steps=grad_accumulation_steps)\n","    quantization_config = BitsAndBytesConfig(\n","        load_in_4bit=True, \n","        bnb_4bit_quant_type=\"nf4\",\n","        bnb_4bit_compute_dtype=torch.bfloat16,\n","        bnb_4bit_use_double_quant=True,\n","    )\n","    \n","    # Load checkpoint\n","    # print(ckpt_path)\n","    model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, \n","                                                 quantization_config=quantization_config, \n","                                                 torch_dtype=torch.bfloat16)\n","    # model.save_pretrained(save_path)\n","    # tokenizer.save_pretrained(save_path)\n","    \n","    \n","    accelerator.print(model)\n","    \n","    peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, \n","                             inference_mode=False, r=r, \n","                             lora_alpha=lora_alpha, lora_dropout=lora_dropout,\n","                             target_modules=\n","                            lora_target_modules_dict.get(MODEL_PATH,\n","                              lora_target_modules_dict['mistralai/Mistral-7B-Instruct-v0.2'],)\n","                            )\n","    peft_model = get_peft_model(model, peft_config)\n","    \n","    if accelerator.is_local_main_process:\n","        peft_model.print_trainable_parameters()\n","    \n","    use_tf32 = True\n","    if use_tf32:\n","      torch.backends.cuda.matmul.allow_tf32 = True\n","      torch.backends.cudnn.allow_tf32 = True\n","    \n","    trainer = SFTTrainer(\n","        model=model,\n","        train_dataset=train,\n","        eval_dataset=test,\n","        args=transformers.TrainingArguments(\n","            output_dir=save_path,\n","            per_device_train_batch_size=batch_size,\n","            per_device_eval_batch_size=4,\n","            gradient_accumulation_steps=grad_accumulation_steps,\n","            gradient_checkpointing=True,\n","            warmup_steps=2,\n","            max_steps=num_epochs,\n","            # num_train_epochs=num_epochs,\n","            load_best_model_at_end=True,\n","            evaluation_strategy='steps',\n","            save_strategy='steps',\n","            eval_steps=eval_steps or checkpointing_steps,\n","            save_steps=checkpointing_steps,\n","            learning_rate=lr,\n","            fp16=True,\n","            tf32=use_tf32,\n","            logging_steps=1,\n","            optim=\"paged_adamw_8bit\",\n","            logging_dir='./logs/',\n","            # save_total_limit=3,\n","            save_only_model=True,\n","        ),\n","        peft_config=peft_config,\n","        formatting_func=formatting_func,\n","        data_collator=collator,\n","    )\n","    resume_from_checkpoint = True\n","    from transformers.trainer_utils import get_last_checkpoint\n","    if get_last_checkpoint(save_path) is None: resume_from_checkpoint = False\n","    trainer.train(resume_from_checkpoint=resume_from_checkpoint,)\n","    \n","    trainer.save_model(save_path)\n","    \n","    print(list(model.parameters())[0][0, 0])\n","    # model = get_peft_model(model, peft_config)\n","    # model = model.merge_and_unload()\n","    # model.save_pretrained(save_path)\n","    # tokenizer.save_pretrained(save_path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["lora_target_modules_dict = {\n","  'gpt2': ['c_attn'],\n","  'distilbert/distilgpt2': ['c_attn'],\n","  'distilbert/distilroberta-base': ['query', 'key', 'value'],\n","  'mistralai/Mistral-7B-Instruct-v0.2': [\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n","}\n","import json\n","os.makedirs('./settings', exist_ok=True)\n","json.dump(lora_target_modules_dict, open('./settings/lora_target_modules.json', 'w'))"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:34:46.442375Z","iopub.status.busy":"2024-03-07T14:34:46.442144Z","iopub.status.idle":"2024-03-07T14:34:46.455275Z","shell.execute_reply":"2024-03-07T14:34:46.454417Z","shell.execute_reply.started":"2024-03-07T14:34:46.442355Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2\n"]}],"source":["if tokenizer.pad_token_id is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","print(tokenizer.pad_token_id)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["import logging\n","import os\n","import datetime\n","os.makedirs('./logs/', exist_ok=True)\n","logging.basicConfig(\n","  level=logging.INFO,\n","  filename='./logs/log_%s.txt' % datetime.datetime.now().strftime('%y%m%d%H%m%S'), filemode='a',\n","  datefmt='%H:%M:%S',\n","  format='%(asctime)s - %(levelname)s - %(message)s',\n",")\n","\n","logger = logging.getLogger(__name__)\n","# Stream handler, logging to the stream\n","logger.info(\"abc\")"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:41:10.458119Z","iopub.status.busy":"2024-03-07T14:41:10.457747Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["./working/trained_models/mistralai/Mistral-7B-Instruct-v0.2/checkpoint.pth\n"]},{"name":"stderr","output_type":"stream","text":["`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.16s/it]\n"]},{"name":"stdout","output_type":"stream","text":["MistralForCausalLM(\n","  (model): MistralModel(\n","    (embed_tokens): Embedding(32000, 4096)\n","    (layers): ModuleList(\n","      (0-31): 32 x MistralDecoderLayer(\n","        (self_attn): MistralSdpaAttention(\n","          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","          (rotary_emb): MistralRotaryEmbedding()\n","        )\n","        (mlp): MistralMLP(\n","          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): MistralRMSNorm()\n","        (post_attention_layernorm): MistralRMSNorm()\n","      )\n","    )\n","    (norm): MistralRMSNorm()\n","  )\n","  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",")\n","trainable params: 83,886,080 || all params: 7,325,618,176 || trainable%: 1.1451058188485088\n"]},{"name":"stderr","output_type":"stream","text":["/home/automl/duckle/eq_gen/venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:225: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n","  warnings.warn(\n","Map: 100%|██████████| 4109/4109 [00:04<00:00, 912.82 examples/s]\n","Map: 100%|██████████| 457/457 [00:00<00:00, 620.01 examples/s]\n","/home/automl/duckle/eq_gen/venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:294: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n","  warnings.warn(\n","`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n","/home/automl/duckle/eq_gen/venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [  2/100 : < :, Epoch 0.02/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>5.303700</td>\n","      <td>5.002770</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# %%capture_text --path \"cap/overfitting.txt\"\n","\n","import os\n","\n","batch_size = 2\n","grad_accumulation_steps = 64\n","num_epochs = 100\n","lr = 5e-5\n","checkpointing_steps = 20\n","eval_steps = 1\n","save_path = os.path.join('./working/trained_models/', MODEL_PATH)\n","r = 32\n","lora_alpha = 32\n","lora_dropout = 0.05\n","\n","# If ckpt_path is a real path (os.path.isfile(ckpt_path) is True),\n","# then the checkpoint will be loaded\n","ckpt_path = os.path.join(save_path, 'checkpoint.pth')\n","print(ckpt_path)\n","kwargs = {\n","'batch_size':batch_size, 'num_epochs':num_epochs, 'lr':lr, 'grad_accumulation_steps':grad_accumulation_steps, \n","'checkpointing_steps':checkpointing_steps, 'save_path':save_path, 'ckpt_path':ckpt_path, 'r':r, 'lora_alpha':lora_alpha, 'lora_dropout':lora_dropout,\n","'eval_steps': eval_steps, \n","}\n","\n","if not os.path.exists(save_path):\n","    os.makedirs(save_path, exist_ok=True)\n","# notebook_launcher(main, args, num_processes=1)\n","main(**kwargs)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def truncate_txt(text, length):\n","    text_list = text.split()\n","    \n","    if len(text_list) <= length:\n","        return text\n","    \n","    return \" \".join(text_list[:length])\n","\n","\n","def gen_style_prompt(og_text, rewritten_text):\n","    \n","    # Truncate the texts to first 200 words for now\n","    # As we are having memory issues on Mixtral8x7b\n","    og_text = truncate_txt(og_text, 200)\n","    rewritten_text = truncate_txt(rewritten_text, 200)\n","    \n","    return f\"\"\"    \n","    Original Essay:\n","    \\\"\"\"{og_text}\\\"\"\"\n","    \n","    Rewritten Essay:\n","    \\\"\"\"{rewritten_text}\\\"\"\"\n","    \n","    You are given 2 essays, the Rewritten essay was created from the Original essay using the google Gemma model.\n","    Analyzing the changes in style, theme, etc., please output the phrase following:\n","    \\\"Please improve the following text using the writing style of [insert author/style name]\\\"\n","    Output should be one sentence only.\n","    \"\"\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","from tqdm import tqdm\n","\n","DEBUG = True\n","\n","TEST_DF_FILE = './input/llm-prompt-recovery/test.csv'\n","SUB_DF_FILE = './input/llm-prompt-recovery/sample_submission.csv'\n","NROWS = None if DEBUG else None\n","\n","if DEBUG:\n","    TEST_DF_FILE = './input/gemma-rewrite-nbroad/nbroad-v1.csv'\n","    SUB_DF_FILE = TEST_DF_FILE\n","\n","tdf = pd.read_csv(TEST_DF_FILE, nrows=NROWS, usecols=['id', 'original_text', 'rewritten_text'])\n","sub = pd.read_csv(SUB_DF_FILE, nrows=NROWS, usecols=['id', 'rewrite_prompt'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["id                                                       LNpAovroGe\n","original_text     This quilt, that my mother made, \\n \\n Still m...\n","rewritten_text    The softest brown and brightest blue quilt, cr...\n","Name: 0, dtype: object"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["tdf.iloc[0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","def chat(prompt, max_new_token=256, **generate_args):\n","\n","  messages = [\n","      {\n","          \"role\": \"user\",\n","          \"content\": prompt,\n","      }\n","  ]\n","  encoded_input = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True, max_length=max_length-1, truncation=True).to('cuda')\n","  print(encoded_input.shape)\n","  print(torch.max(encoded_input), tokenizer.vocab_size)\n","  with torch.no_grad():\n","      encoded_output = model.generate(encoded_input, max_new_tokens=max_new_token, do_sample=True, pad_token_id=tokenizer.eos_token_id,\n","                                      **generate_args)\n","  \n","  decoded_output = tokenizer.batch_decode(encoded_output, skip_special_tokens=True)[0]\n","  decoded_output = re.sub(r\"[\\s\\S]*\\[\\/INST\\]\", '', decoded_output, 1).replace(prompt, '')\n","  \n","  return decoded_output"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["628"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["i = 2\n","# prompt = (f\"Give me the prompt used to have the rewritten text from the original text.\\nORIGINAL TEXT:\\n{data['original_text'][i]}\\n\\nREWRITTEN TEXT:\\n{data['rewritten_text'][i]}\\n\\nPROMPT:\\n\")\n","prompt = gen_prompt(data['original_text'][i], data['rewritten_text'][i])\n","len(tokenizer.encode(prompt))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Grimm's Fairy Tales: Adapt the text to mimic the tone and style of the Brothers Grimm's fairy tales, often dark with moral lessons.\n"]}],"source":["print(data['rewrite_prompt'][i])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Instruction:\n","    You are given 2 essays, the Rewritten text was created from the Original text using the google Gemma model.\n","    Analyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten text.\n","    Start directly with the prompt, output should be one line only.\n","    \n","    Original Text:\n","    \"\"\"The first punch gets me right in the ribs, knocking the wind out of me. I half-crawl towards him from where I fell on my ass, feeling the flush of adrenaline starting to take hold. It's a dirty fight, I suppose. We focus on causing as much pain as possible - cruel to be cruel. He stinks. ******** The actual first punch was somewhat expected. The sun was hot, uncomfortably so, but we felt obligated to enjoy it. The ground in our neighbourhood was both more and less interesting in the summer. Deep cracks and canyons traced out a huge map that stretched from his backyard to the fabled danger-zone of the main road, but it lacked the fascination of living and moving insects, driven underground by drought. What else could two boys do, but fight? It was fun. We laughed afterwards. We compared bruises for weeks; his turned yellow first. If there was blood, I do n't remember it. We both of stunk afterwards. Of sweat and dirt and pure heat. ******** He's lost weight - or, rather, is in the process of losing it. The bones in his wrists are becoming visible. It's summer again, not that you\"\"\"\n","    \n","    Rewritten Text:\n","    \"\"\"In the sweltering sun, the stench of sweat and battle permeated the air. A brutal encounter unfolded beneath the blazing sky, a testament to the folly of youth and the sinister nature of unchecked aggression. The punch, swift and powerful, left me reeling in agony. I stumbled towards him, my breath expelled in a strangled gasp. The fight, a dance of pain and desperation, was a cruel spectacle. His stench, a potent concoction of sweat and despair, filled my nostrils, a pungent reminder of the consequences of his actions. The ground beneath us was scorched and barren, devoid of the beauty that once adorned it. As we exchanged blows, the sun cast long shadows across the barren canvas, a grim backdrop for our folly. The air was thick with the scent of sweat and violence, a testament to the intensity of our battle. In the aftermath of the fight, we both reeked of sweat and dirt. His wounds, superficial yet agonizing, began to heal, a testament to the fleeting nature of youth and the enduring power of forgiveness. But in that moment, there was no room for compassion or remorse. As the sun dipped behind the horizon, casting long shadows\"\"\"\n","    \n","    Prompt:\n"]}],"source":["print(prompt)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch.cuda.empty_cache()\n","import gc\n","gc.collect(2)\n","try: del model\n","except: pass"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6168afff2e5e46adb6fbef3f420923ae","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 1000.00 MiB. GPU 0 has a total capacity of 3.81 GiB of which 15.19 MiB is free. Including non-PyTorch memory, this process has 3.78 GiB memory in use. Of the allocated memory 3.64 GiB is allocated by PyTorch, and 37.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[34], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m quantization_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m      2\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# bnb_4bit_quant_type=\"nf4\",\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# bnb_4bit_use_double_quant=True,\u001b[39;00m\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgoogle/gemma-2b-it\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m chat(prompt, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/media/minhduck/SSD1/Documents/projects/pyenvs/rlhf/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:561\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    560\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m )\n","File \u001b[0;32m/media/minhduck/SSD1/Documents/projects/pyenvs/rlhf/lib/python3.10/site-packages/transformers/modeling_utils.py:3502\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3494\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3495\u001b[0m     (\n\u001b[1;32m   3496\u001b[0m         model,\n\u001b[1;32m   3497\u001b[0m         missing_keys,\n\u001b[1;32m   3498\u001b[0m         unexpected_keys,\n\u001b[1;32m   3499\u001b[0m         mismatched_keys,\n\u001b[1;32m   3500\u001b[0m         offload_index,\n\u001b[1;32m   3501\u001b[0m         error_msgs,\n\u001b[0;32m-> 3502\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3506\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3509\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3510\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3513\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3514\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3518\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3520\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   3521\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n","File \u001b[0;32m/media/minhduck/SSD1/Documents/projects/pyenvs/rlhf/lib/python3.10/site-packages/transformers/modeling_utils.py:3926\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3924\u001b[0m                     hf_quantizer\u001b[38;5;241m.\u001b[39mcreate_quantized_param(model, param, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, state_dict)\n\u001b[1;32m   3925\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3926\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3927\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3928\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3929\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloaded_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3930\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3931\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3932\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3933\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3934\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3935\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3936\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3937\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3938\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3939\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3940\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3941\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3942\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3943\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   3944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/media/minhduck/SSD1/Documents/projects/pyenvs/rlhf/lib/python3.10/site-packages/transformers/modeling_utils.py:805\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys)\u001b[0m\n\u001b[1;32m    798\u001b[0m     state_dict_index \u001b[38;5;241m=\u001b[39m offload_weight(param, param_name, model, state_dict_folder, state_dict_index)\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    800\u001b[0m     hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m hf_quantizer\u001b[38;5;241m.\u001b[39mrequires_parameters_quantization)\n\u001b[1;32m    802\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m hf_quantizer\u001b[38;5;241m.\u001b[39mcheck_quantized_param(model, param, param_name, state_dict))\n\u001b[1;32m    803\u001b[0m ):\n\u001b[1;32m    804\u001b[0m     \u001b[38;5;66;03m# For backward compatibility with older versions of `accelerate` and for non-quantized params\u001b[39;00m\n\u001b[0;32m--> 805\u001b[0m     \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mset_module_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    807\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mcreate_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n","File \u001b[0;32m/media/minhduck/SSD1/Documents/projects/pyenvs/rlhf/lib/python3.10/site-packages/accelerate/utils/modeling.py:384\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    382\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 384\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1000.00 MiB. GPU 0 has a total capacity of 3.81 GiB of which 15.19 MiB is free. Including non-PyTorch memory, this process has 3.78 GiB memory in use. Of the allocated memory 3.64 GiB is allocated by PyTorch, and 37.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"]}],"source":["quantization_config = BitsAndBytesConfig(\n","    load_in_4bit=True, \n","    # bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16,\n","    # bnb_4bit_use_double_quant=True,\n",")\n","\n","model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, \n","                                                 quantization_config=quantization_config, \n","                                                 torch_dtype=torch.float16\n","                                                 )\n","chat(prompt, top_k=1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"]},{"name":"stdout","output_type":"stream","text":["torch.Size([1, 629])\n","tensor(50256, device='cuda:0') 50257\n"]},{"data":{"text/plain":["'Instruction:\\n    You are given 2 essays, the Rewritten text was created from the Original text using the google Gemma model.\\n    Analyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten text.\\n    Start directly with the prompt, output should be one line only.\\n    \\n    Original Text:\\n    \"\"\"The first punch gets me right in the ribs, knocking the wind out of me. I half-crawl towards him from where I fell on my ass, feeling the flush of adrenaline starting to take hold. It\\'s a dirty fight, I suppose. We focus on causing as much pain as possible - cruel to be cruel. He stinks. ******** The actual first punch was somewhat expected. The sun was hot, uncomfortably so, but we felt obligated to enjoy it. The ground in our neighbourhood was both more and less interesting in the summer. Deep cracks and canyons traced out a huge map that stretched from his backyard to the fabled danger-zone of the main road, but it lacked the fascination of living and moving insects, driven underground by drought. What else could two boys do, but fight? It was fun. We laughed afterwards. We compared bruises for weeks; his turned yellow first. If there was blood, I don\\'t remember it. We both of stunk afterwards. Of sweat and dirt and pure heat. ******** He\\'s lost weight - or, rather, is in the process of losing it. The bones in his wrists are becoming visible. It\\'s summer again, not that you\"\"\"\\n    \\n    Rewritten Text:\\n    \"\"\"In the sweltering sun, the stench of sweat and battle permeated the air. A brutal encounter unfolded beneath the blazing sky, a testament to the folly of youth and the sinister nature of unchecked aggression. The punch, swift and powerful, left me reeling in agony. I stumbled towards him, my breath expelled in a strangled gasp. The fight, a dance of pain and desperation, was a cruel spectacle. His stench, a potent concoction of sweat and despair, filled my nostrils, a pungent reminder of the consequences of his actions. The ground beneath us was scorched and barren, devoid of the beauty that once adorned it. As we exchanged blows, the sun cast long shadows across the barren canvas, a grim backdrop for our folly. The air was thick with the scent of sweat and violence, a testament to the intensity of our battle. In the aftermath of the fight, we both reeked of sweat and dirt. His wounds, superficial yet agonizing, began to heal, a testament to the fleeting nature of youth and the enduring power of forgiveness. But in that moment, there was no room for compassion or remorse. As the sun dipped behind the horizon, casting long shadows\"\"\"\\n    \\n    Prompt:The first time I saw the book, I was shocked. It was a book that had been in my living room for years, but I was still shocked by the way it looked.\\n    \\n   \\n    The book was a masterpiece, but it was also a bit of a disappointment.    \\n  \\n    The book was a masterpiece, but it was also a disappointment.    \\n   \\n    The book was a masterpiece, but it was also a disappointment.    \\n   \\n    The book was a masterpiece, but it was also a disappointment.    \\n   \\n    The book was a masterpiece, but it was also a disappointment.    \\n   \\n    The book was a masterpiece, but it was also a disappointment.    \\n   \\n     The book was a masterpiece, but it was also a disappointment.    \\n  \\n     The book was a masterpiece, but it was also a disappointment.    \\n  \\n     The book was'"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["quantization_config = BitsAndBytesConfig(\n","    load_in_4bit=True, \n","    # bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16,\n","    # bnb_4bit_use_double_quant=True,\n",")\n","\n","model = AutoModelForCausalLM.from_pretrained(save_path, \n","                                                 quantization_config=quantization_config, \n","                                                 torch_dtype=torch.float16\n","                                                 )\n","chat(prompt, top_k=1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"]},{"name":"stdout","output_type":"stream","text":["torch.Size([1, 634])\n","tensor(50256, device='cuda:0') 50257\n"]}],"source":["res = chat(prompt, top_k = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"IndexError","evalue":"list index out of range","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPrompt:\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}],"source":["res.split('Prompt:')[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:40:45.855872Z","iopub.status.busy":"2024-03-07T14:40:45.855499Z","iopub.status.idle":"2024-03-07T14:40:46.067760Z","shell.execute_reply":"2024-03-07T14:40:46.066318Z","shell.execute_reply.started":"2024-03-07T14:40:45.855842Z"},"trusted":true},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-07T14:37:47.408644Z","iopub.status.idle":"2024-03-07T14:37:47.408985Z","shell.execute_reply":"2024-03-07T14:37:47.408836Z","shell.execute_reply.started":"2024-03-07T14:37:47.408821Z"},"trusted":true},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import time\n","time.sleep(1000)"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":7806901,"sourceId":67121,"sourceType":"competition"},{"datasetId":4506214,"sourceId":7747717,"sourceType":"datasetVersion"},{"sourceId":140952720,"sourceType":"kernelVersion"},{"sourceId":164836055,"sourceType":"kernelVersion"},{"modelInstanceId":284,"sourceId":386,"sourceType":"modelInstanceVersion"},{"modelInstanceId":3900,"sourceId":5112,"sourceType":"modelInstanceVersion"},{"modelInstanceId":4761,"sourceId":5994,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelInstanceId":8332,"sourceId":11394,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
