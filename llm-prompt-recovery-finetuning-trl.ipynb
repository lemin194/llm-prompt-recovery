{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-03-07T14:34:34.653535Z","iopub.status.busy":"2024-03-07T14:34:34.652850Z","iopub.status.idle":"2024-03-07T14:34:40.781469Z","shell.execute_reply":"2024-03-07T14:34:40.780703Z","shell.execute_reply.started":"2024-03-07T14:34:34.653491Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/automl/duckle/eq_gen/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["from accelerate.utils import BnbQuantizationConfig\n","from accelerate import Accelerator, notebook_launcher\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, \\\n","                        get_cosine_schedule_with_warmup, set_seed\n","import transformers\n","import optimum\n","\n","from datasets import load_dataset,Dataset\n","import pandas as pd\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, \\\n","                        get_cosine_schedule_with_warmup, set_seed\n","from peft import LoraConfig, TaskType, get_peft_model\n","from accelerate import Accelerator, notebook_launcher\n","from accelerate.utils import set_seed\n","import logging\n","from torch.utils.data import DataLoader\n","from torch.optim import AdamW, SGD\n","from tqdm.notebook import tqdm\n","import torch\n","from torch.nn.utils.rnn import pad_sequence\n","import glob\n","from collections import OrderedDict\n","import re\n","\n","import os\n","\n","import pandas as pd\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:34:40.783110Z","iopub.status.busy":"2024-03-07T14:34:40.782634Z","iopub.status.idle":"2024-03-07T14:34:40.787187Z","shell.execute_reply":"2024-03-07T14:34:40.786265Z","shell.execute_reply.started":"2024-03-07T14:34:40.783080Z"},"trusted":true},"outputs":[],"source":["import datetime\n","start_time = datetime.datetime.now()"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def gen_train_test():\n","  df = pd.concat([\n","    # pd.read_csv(\"./input/gemma-rewrite-nbroad/nbroad-v1.csv\"),\n","    # pd.read_csv(\"./input/gemma-rewrite-nbroad/nbroad-v2.csv\"),\n","    pd.read_csv(\"./input/mydata/train_part1.csv\"),\n","  ]).reset_index(drop=True)\n","\n","  from sklearn.model_selection import train_test_split\n","\n","  train, test = train_test_split(df, test_size=0.1)\n","  # train.to_csv('./input/gemma-rewrite-nbroad/train.csv')\n","  # test.to_csv('./input/gemma-rewrite-nbroad/test.csv')\n","  train.to_csv('./input/mydata/train.csv')\n","  test.to_csv('./input/mydata/test.csv')"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:34:42.360542Z","iopub.status.busy":"2024-03-07T14:34:42.360264Z","iopub.status.idle":"2024-03-07T14:34:46.357229Z","shell.execute_reply":"2024-03-07T14:34:46.356439Z","shell.execute_reply.started":"2024-03-07T14:34:42.360518Z"},"trusted":true},"outputs":[],"source":["# MODEL_PATH = \"/kaggle/input/gemma/transformers/7b-it/2\"\n","# MODEL_PATH = \"distilbert/distilgpt2\"\n","# MODEL_PATH = \"gpt2\"\n","# MODEL_PATH = \"microsoft/deberta-v3-base\"\n","# MODEL_PATH = \"distilbert/distilroberta-base\"\n","MODEL_PATH = 'mistralai/Mistral-7B-Instruct-v0.2'\n","# MODEL_PATH = \"google/gemma-2b-it\"\n","# MODEL_PATH = \"google/flan-t5-small\"\n","# MODEL_PATH = \"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\"\n","# MODEL_PATH = \"/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1\"\n","# MODEL_PATH = \"/kaggle/input/llama-2/pytorch/7b-chat-hf/1\"\n","# MODEL_PATH = \"/kaggle/input/llama-2/pytorch/13b-chat-hf/1\"\n","\n","max_length = 1024\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n","tokenizer.model_max_length = max(tokenizer.model_max_length, max_length)\n","\n","    "]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 4109/4109 [00:04<00:00, 927.53 examples/s] \n","Map: 100%|██████████| 4109/4109 [00:02<00:00, 1379.21 examples/s]\n","Map: 100%|██████████| 4109/4109 [00:00<00:00, 15164.30 examples/s]\n","Map: 100%|██████████| 457/457 [00:00<00:00, 1187.83 examples/s]\n","Map: 100%|██████████| 457/457 [00:00<00:00, 2059.82 examples/s]\n","Map: 100%|██████████| 457/457 [00:00<00:00, 15362.52 examples/s]"]},{"name":"stdout","output_type":"stream","text":["4109 457\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["def create_data(df, tokenizer, split='train'):\n","  data = Dataset.from_pandas(df[[\n","    'rewrite_prompt', 'original_text', 'rewritten_text'\n","  ]],split=split)\n","\n","\n","  # def tokenize_samples(samples):\n","  #   inputs = tokenizer(samples[\"reverse_prompt\"], max_length=max_length, truncation=True)\n","  #   targets = tokenizer(samples[\"rewrite_prompt\"], max_length=max_length, truncation=True)\n","  #   return {'input_ids': inputs['input_ids'], 'attention_mask': inputs['attention_mask'], 'labels': targets['input_\n","  return data\n","\n","train, test = (\n","  create_data(pd.read_csv('./input/gemma-rewrite-nbroad/train.csv'), tokenizer, 'train'),\n","  create_data(pd.read_csv('./input/gemma-rewrite-nbroad/test.csv'), tokenizer, 'test'),\n",")\n","print(len(train), len(test))"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["\n","def truncate_txt(text, length):\n","    text_list = text.split()\n","    \n","    if len(text_list) <= length:\n","        return text\n","    \n","    return \" \".join(text_list[:length])\n","\n","\n","def gen_prompt(og_text, rewritten_text, truncate_length=200):\n","    \n","    # Truncate the texts to first 200 words for now\n","    # As we are having memory issues on Mixtral8x7b\n","    og_text = truncate_txt(og_text, truncate_length)\n","    rewritten_text = truncate_txt(rewritten_text, truncate_length)\n","    \n","    return f\"\"\"\n","You are given 2 essays, the Rewritten essay was created from the Original essay using the google Gemma model.\n","Analyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten essay.\n","Start directly with the prompt, output should be one line only.\n","\n","Original Essay:\n","\\\"\"\"{og_text}\\\"\"\"\n","\n","Rewritten Essay:\n","\\\"\"\"{rewritten_text}\\\"\"\"\n","\n","\"\"\".strip()\n","\n","def formatting_func(example):\n","  output_texts = []\n","  for i in range(len(example['original_text'])):\n","    prompt = tokenizer.apply_chat_template(\n","      [{\n","        'role': 'user',\n","        'content' : gen_prompt(example['original_text'][i], example['rewritten_text'][i]) + '\\n ',\n","      }],\n","      tokenize=False,\n","    )\n","    text = f\"{prompt}{example['rewrite_prompt'][i]}{tokenizer.eos_token}\"\n","    output_texts.append(text)\n","  return output_texts"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:41:09.200753Z","iopub.status.busy":"2024-03-07T14:41:09.200368Z","iopub.status.idle":"2024-03-07T14:41:09.223448Z","shell.execute_reply":"2024-03-07T14:41:09.222511Z","shell.execute_reply.started":"2024-03-07T14:41:09.200725Z"},"trusted":true},"outputs":[],"source":["from trl import SFTTrainer\n","\n","def main(batch_size: int, num_epochs: int, lr: float, grad_accumulation_steps: int, \n","         checkpointing_steps: int, save_path: str, ckpt_path: str,\n","         num_warmup_steps: int=0, r: int=4, lora_alpha: int=32, lora_dropout: float=0.1,\n","         eval_steps=None):\n","    set_seed(1234)\n","    \n","    accelerator = Accelerator(gradient_accumulation_steps=grad_accumulation_steps)\n","    quantization_config = BitsAndBytesConfig(\n","        load_in_4bit=True, \n","        bnb_4bit_quant_type=\"nf4\",\n","        bnb_4bit_compute_dtype=torch.bfloat16,\n","        bnb_4bit_use_double_quant=True,\n","    )\n","    \n","    # Load checkpoint\n","    # print(ckpt_path)\n","    model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, \n","                                                 quantization_config=quantization_config, \n","                                                 torch_dtype=torch.bfloat16)\n","    # model.save_pretrained(save_path)\n","    # tokenizer.save_pretrained(save_path)\n","    \n","    \n","    accelerator.print(model)\n","    \n","    peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, \n","                             inference_mode=False, r=r, \n","                             lora_alpha=lora_alpha, lora_dropout=lora_dropout,\n","                             target_modules=\n","                            lora_target_modules_dict.get(MODEL_PATH,\n","                              lora_target_modules_dict['mistralai/Mistral-7B-Instruct-v0.2'],)\n","                            )\n","    peft_model = get_peft_model(model, peft_config)\n","    \n","    if accelerator.is_local_main_process:\n","        peft_model.print_trainable_parameters()\n","    \n","    use_tf32 = True\n","    if use_tf32:\n","      torch.backends.cuda.matmul.allow_tf32 = True\n","      torch.backends.cudnn.allow_tf32 = True\n","    \n","    trainer = SFTTrainer(\n","        model=model,\n","        train_dataset=train,\n","        eval_dataset=test,\n","        args=transformers.TrainingArguments(\n","            output_dir=save_path,\n","            per_device_train_batch_size=batch_size,\n","            per_device_eval_batch_size=4,\n","            gradient_accumulation_steps=grad_accumulation_steps,\n","            gradient_checkpointing=True,\n","            warmup_steps=2,\n","            max_steps=num_epochs,\n","            # num_train_epochs=num_epochs,\n","            load_best_model_at_end=True,\n","            evaluation_strategy='steps',\n","            save_strategy='steps',\n","            eval_steps=eval_steps or checkpointing_steps,\n","            save_steps=checkpointing_steps,\n","            learning_rate=lr,\n","            fp16=True,\n","            tf32=use_tf32,\n","            logging_steps=1,\n","            optim=\"paged_adamw_8bit\",\n","            logging_dir='./logs/',\n","            # save_total_limit=3,\n","            save_only_model=True,\n","        ),\n","        peft_config=peft_config,\n","        formatting_func=formatting_func,\n","        data_collator=collator,\n","    )\n","    resume_from_checkpoint = True\n","    from transformers.trainer_utils import get_last_checkpoint\n","    if get_last_checkpoint(save_path) is None: resume_from_checkpoint = False\n","    trainer.train(resume_from_checkpoint=resume_from_checkpoint,)\n","    \n","    trainer.save_model(save_path)\n","    \n","    print(list(model.parameters())[0][0, 0])\n","    # model = get_peft_model(model, peft_config)\n","    # model = model.merge_and_unload()\n","    # model.save_pretrained(save_path)\n","    # tokenizer.save_pretrained(save_path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["lora_target_modules_dict = {\n","  'gpt2': ['c_attn'],\n","  'FacebookAI/roberta-base': ['query', 'key', 'value'],\n","  'mistralai/Mistral-7B-Instruct-v0.2': [\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n","}\n","import json\n","os.makedirs('./settings', exist_ok=True)\n","json.dump(lora_target_modules_dict, open('./settings/lora_target_modules.json', 'w'))"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:34:46.442375Z","iopub.status.busy":"2024-03-07T14:34:46.442144Z","iopub.status.idle":"2024-03-07T14:34:46.455275Z","shell.execute_reply":"2024-03-07T14:34:46.454417Z","shell.execute_reply.started":"2024-03-07T14:34:46.442355Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2\n"]}],"source":["if tokenizer.pad_token_id is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","print(tokenizer.pad_token_id)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["import logging\n","import os\n","import datetime\n","os.makedirs('./logs/', exist_ok=True)\n","logging.basicConfig(\n","  level=logging.INFO,\n","  filename='./logs/log_%s.txt' % datetime.datetime.now().strftime('%y%m%d%H%m%S'), filemode='a',\n","  datefmt='%H:%M:%S',\n","  format='%(asctime)s - %(levelname)s - %(message)s',\n",")\n","\n","logger = logging.getLogger(__name__)\n","# Stream handler, logging to the stream\n","logger.info(\"abc\")"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T14:41:10.458119Z","iopub.status.busy":"2024-03-07T14:41:10.457747Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["./working/trained_models/mistralai/Mistral-7B-Instruct-v0.2/checkpoint.pth\n"]},{"name":"stderr","output_type":"stream","text":["`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.16s/it]\n"]},{"name":"stdout","output_type":"stream","text":["MistralForCausalLM(\n","  (model): MistralModel(\n","    (embed_tokens): Embedding(32000, 4096)\n","    (layers): ModuleList(\n","      (0-31): 32 x MistralDecoderLayer(\n","        (self_attn): MistralSdpaAttention(\n","          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","          (rotary_emb): MistralRotaryEmbedding()\n","        )\n","        (mlp): MistralMLP(\n","          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): MistralRMSNorm()\n","        (post_attention_layernorm): MistralRMSNorm()\n","      )\n","    )\n","    (norm): MistralRMSNorm()\n","  )\n","  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",")\n","trainable params: 83,886,080 || all params: 7,325,618,176 || trainable%: 1.1451058188485088\n"]},{"name":"stderr","output_type":"stream","text":["/home/automl/duckle/eq_gen/venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:225: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n","  warnings.warn(\n","Map: 100%|██████████| 4109/4109 [00:04<00:00, 912.82 examples/s]\n","Map: 100%|██████████| 457/457 [00:00<00:00, 620.01 examples/s]\n","/home/automl/duckle/eq_gen/venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:294: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n","  warnings.warn(\n","`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n","/home/automl/duckle/eq_gen/venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [  2/100 : < :, Epoch 0.02/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>5.303700</td>\n","      <td>5.002770</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# %%capture_text --path \"cap/overfitting.txt\"\n","\n","import os\n","\n","batch_size = 2\n","grad_accumulation_steps = 64\n","num_epochs = 100\n","lr = 5e-5\n","checkpointing_steps = 20\n","eval_steps = 1\n","save_path = os.path.join('./working/trained_models/', MODEL_PATH)\n","r = 32\n","lora_alpha = 32\n","lora_dropout = 0.05\n","\n","# If ckpt_path is a real path (os.path.isfile(ckpt_path) is True),\n","# then the checkpoint will be loaded\n","ckpt_path = os.path.join(save_path, 'checkpoint.pth')\n","print(ckpt_path)\n","kwargs = {\n","'batch_size':batch_size, 'num_epochs':num_epochs, 'lr':lr, 'grad_accumulation_steps':grad_accumulation_steps, \n","'checkpointing_steps':checkpointing_steps, 'save_path':save_path, 'ckpt_path':ckpt_path, 'r':r, 'lora_alpha':lora_alpha, 'lora_dropout':lora_dropout,\n","'eval_steps': eval_steps, \n","}\n","\n","if not os.path.exists(save_path):\n","    os.makedirs(save_path, exist_ok=True)\n","# notebook_launcher(main, args, num_processes=1)\n","main(**kwargs)"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":7806901,"sourceId":67121,"sourceType":"competition"},{"datasetId":4506214,"sourceId":7747717,"sourceType":"datasetVersion"},{"sourceId":140952720,"sourceType":"kernelVersion"},{"sourceId":164836055,"sourceType":"kernelVersion"},{"modelInstanceId":284,"sourceId":386,"sourceType":"modelInstanceVersion"},{"modelInstanceId":3900,"sourceId":5112,"sourceType":"modelInstanceVersion"},{"modelInstanceId":4761,"sourceId":5994,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelInstanceId":8332,"sourceId":11394,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
